# Project Requirements Document
## RL-Enhanced Model Predictive Control with Sequential Transfer Learning for Multi-UAV Systems

**Project Title:** Reinforcement Learning-Enhanced Model Predictive Control Using Bryson's Rule Initialization and Sequential Transfer Learning Across Heterogeneous Drone Platforms

**Author:** Dr. Abdul Manan Khan  
**Institution:** University of West London  
**Date:** November 2025  
**Version:** 2.0

**Based on:** Published research in AAAI Conference proceedings demonstrating 75% reduction in training steps and 56.2% reduction in training time through sequential transfer learning across 200Ã— mass variation.

---

## Executive Summary

This project builds upon published research that successfully demonstrated RL-enhanced MPC with sequential transfer learning across heterogeneous UAV systems. The framework extends this work by: (1) incorporating Bryson's Rule for intelligent MPC weight initialization, (2) implementing multiple RL algorithms including policy gradient methods (PPO, TRPO) and actor-critic methods (SAC, TD3, A2C), and (3) validating transfer learning across four UAV platforms ranging from 0.027kg to 5.5kg (200Ã— mass variation). 

The original research achieved consistent 1.34Â±0.01m tracking error across all platforms with 75% reduction in training steps (20,000 â†’ 5,000) and 56.2% total training time reduction (801 min â†’ 363 min) through sequential transfer learning. This extended framework will compare multiple RL algorithms, incorporate Bryson's Rule-based initialization for faster convergence, and provide comprehensive analysis of which approaches work best for different platform characteristics.

---

## 2. Core Research Contribution

### 2.1 The Problem with Expert Manual MPC Tuning

**Traditional Approach:**
When deploying MPC controllers across multiple drone platforms, the standard practice is:

1. **Platform 1 (Crazyflie):** Expert spends 4-8 hours manually tuning Q, R, N
2. **Platform 2 (Racing):** Expert spends another 4-8 hours tuning from scratch
3. **Platform 3 (Generic):** Another 4-8 hours of manual tuning
4. **Platform 4 (Heavy-Lift):** Yet another 4-8 hours of tuning

**Total time:** 16-32 hours of expert labor
**Total cost:** Expert engineer salary Ã— 16-32 hours
**Results:** Inconsistent performance across platforms (1.5-2.2m RMSE range)

**Key Problems:**
- **No knowledge transfer:** Each platform requires complete re-tuning
- **Time-consuming:** 16-32 hours total for 4 platforms
- **Expertise bottleneck:** Requires deep MPC and control theory knowledge
- **Inconsistent results:** Performance varies significantly by platform
- **Not scalable:** Adding more platforms linearly increases expert time
- **Suboptimal:** Manual tuning often settles for "good enough" rather than optimal

### 2.2 Our Proposed Solution: RL + Bryson's Rule + Transfer Learning

**Revolutionary Approach:**

**Step 1: Intelligent Initialization (Bryson's Rule)**
- Apply analytical formula to get physically meaningful starting weights
- Takes ~10 minutes per platform
- Provides "good" baseline (2-3m RMSE) before any optimization

**Step 2: Base Platform RL Training (Once)**
- Train PPO on Crazyflie with Bryson's initialization
- 20,000 steps, 200 minutes (3.3 hours)
- Achieves 1.33m RMSE (better than expert manual tuning)

**Step 3: Sequential Transfer to Other Platforms**
- Transfer learned policy to Racing â†’ fine-tune 5,000 steps (52 min)
- Transfer to Generic â†’ fine-tune 5,000 steps (52 min)
- Transfer to Heavy-Lift â†’ fine-tune 5,000 steps (59 min)

**Total time:** 6.1 hours automated training
**Total cost:** Compute cost on consumer hardware ($50-100 electricity)
**Results:** Consistent 1.34Â±0.01m RMSE across all platforms

### 2.3 Value Proposition

#### Time Savings
```
Expert Manual Tuning:  16-32 hours (pessimistic to optimistic)
RL + Transfer:         6.1 hours
Savings:               62-81% time reduction
```

#### Cost Savings
```
Expert labor (@ $100/hr): $1,600 - $3,200
Compute cost:             $50 - $100
Savings:                  $1,500 - $3,100 per deployment
```

#### Performance Improvement
```
Expert RMSE:        1.5-2.2m (platform-dependent)
RL + Transfer:      1.34Â±0.01m (consistent)
Improvement:        11-47% better tracking
```

#### Consistency
```
Expert std deviation:  Â±0.3-0.4m across platforms
RL + Transfer:         Â±0.01m across platforms
Improvement:           30-40Ã— more consistent
```

#### Scalability
```
Expert: Linear scaling (8h Ã— N platforms)
RL + Transfer: Sub-linear scaling (3.3h base + 1h Ã— N platforms)

For 10 platforms:
  Expert: 80 hours
  RL + Transfer: 12.3 hours
  Savings: 85%
```

#### Democratization
```
Expert manual: Requires PhD/MSc in control + 3-5 years experience
RL + Transfer: Requires basic understanding of running training scripts
Benefit: Opens MPC tuning to software engineers, not just control experts
```

### 2.4 Why This Matters

**For Industry:**
- Deploy optimized MPC controllers across heterogeneous drone fleets in hours, not weeks
- No need to hire expensive control theory experts
- Consistent, predictable performance across all platforms
- Easy to add new platforms (just 1 hour of fine-tuning)

**For Research:**
- Demonstrates RL can outperform expert manual tuning on real control problems
- Shows transfer learning works across 200Ã— mass variation
- Validates Bryson's Rule as intelligent initialization for RL
- Provides open-source framework for reproducing results

**For Practitioners:**
- Turns MPC tuning from "black art" into automated engineering process
- Makes advanced control accessible to non-experts
- Provides pre-trained models for common drone platforms
- Enables rapid prototyping and testing

### 2.5 Key Innovation: Bryson's Rule + RL Synergy

**Bryson's Rule alone:** Fast (10 min) but suboptimal (2-3m RMSE)
**RL alone:** Optimal (1.34m) but slow to explore (25,000+ steps)
**Bryson's + RL (Ours):** Optimal (1.34m) with faster convergence (20,000 steps)

**The Synergy:**
1. Bryson's Rule provides physically meaningful initialization
2. RL starts in "good region" of parameter space, not random
3. RL fine-tunes from good starting point to optimal
4. Learned policy transfers to new platforms because Bryson's scaling captured physics
5. Fine-tuning adjusts for platform-specific dynamics efficiently

**Result:** Best of both worlds - speed of analytical methods + optimality of learning

---

## 1. Project Objectives

### 1.1 Primary Objectives

**Main Goal:** Demonstrate that RL-enhanced MPC tuning with Bryson's Rule initialization and sequential transfer learning is superior to traditional expert manual tuning across heterogeneous UAV platforms.

**Specific Objectives:**

1. **Establish Expert Manual Tuning Baseline**
   - Document expert tuning process and time requirements (4-8 hours per platform)
   - Measure expert-achieved RMSE performance (expected: 1.5-2.2m range)
   - Quantify consistency across platforms
   - Identify challenges and limitations of manual approach

2. **Implement RL-Enhanced MPC Framework**
   - Develop nonlinear MPC controller using CasADi and IPOPT
   - Implement Bryson's Rule for intelligent weight initialization  
   - Create RL training pipeline with multiple algorithms (PPO, TRPO, SAC, TD3, A2C)
   - Build sequential transfer learning system

3. **Validate Performance Across 4 Platforms**
   - Train on base platform (Crazyflie, 0.027kg)
   - Transfer and fine-tune on Racing (0.800kg), Generic (2.500kg), Heavy-Lift (5.500kg)
   - Achieve target: 1.34Â±0.01m RMSE across 200Ã— mass variation
   - Complete training in â‰¤6.1 hours total

4. **Demonstrate Superior Efficiency**
   - Show 62-81% time savings vs expert manual tuning (6.1h vs 16-32h)
   - Achieve 75% training step reduction via transfer learning (5k vs 20k)
   - Prove 56.2% total training time reduction (363 min vs 801 min without transfer)

5. **Prove Better Performance**
   - Outperform expert manual tuning by 11-47% in RMSE
   - Achieve 30-40Ã— better consistency across platforms (Â±0.01m vs Â±0.3-0.4m)
   - Maintain stable performance across diverse trajectory types

6. **Compare RL Algorithms**
   - Evaluate 5 algorithms: PPO (proven), TRPO, SAC, TD3, A2C
   - Identify best algorithm for MPC hyperparameter tuning
   - Compare sample efficiency, wall-clock time, and transferability

### 1.2 Secondary Objectives
1. Establish benchmark performance metrics achieving 1.34Â±0.01m tracking error across all platforms
2. Quantify sample efficiency and wall-clock training time across different RL algorithms
3. Evaluate generalization capabilities across four distinct UAV platforms (Crazyflie, Racing, Generic, Heavy-Lift)
4. Develop automated checkpoint-based training pipeline resilient to interruptions
5. Compare Bryson's Rule initialization against random initialization and Bayesian Optimization
6. Provide reusable framework for MPC-RL integration in heterogeneous multi-agent systems

### Research Questions (Corrected)

**Primary Research Question:**
How does RL-based MPC tuning with Bryson's Rule initialization and transfer learning compare to traditional expert manual tuning across heterogeneous drone platforms?

**Specific Research Questions:**

1. **Expert vs. RL-Enhanced Tuning:**
   - How does RL-tuned MPC (trained on one drone, transferred to others) compare to expert manual tuning for each individual platform?
   - What is the time savings: RL transfer (5,000 steps, ~50 min) vs. expert tuning (4-8 hours per platform)?
   - Which approach achieves better tracking performance?

2. **Bryson's Rule as Intelligent Initialization:**
   - How does Bryson's Rule initialization accelerate RL convergence compared to random initialization?
   - Can Bryson's Rule provide "good enough" baseline performance before RL optimization?
   - Does Bryson's Rule make the RL-learned weights more interpretable and physically meaningful?

3. **Transfer Learning Efficiency:**
   - Can MPC parameters learned on a lightweight drone (Crazyflie, 0.027kg) effectively transfer to a heavy drone (5.5kg) with minimal fine-tuning?
   - How much fine-tuning is needed: 25% of baseline training (5,000 steps) sufficient?
   - Does sequential transfer (Crazyflie â†’ Racing â†’ Generic â†’ Heavy) work better than direct transfer (Crazyflie â†’ Heavy)?

4. **Optimality Comparison:**
   - Does RL-tuned MPC find better hyperparameters than expert tuning can achieve?
   - Are the learned parameters stable and consistent across different trajectory types?
   - How does the 1.34Â±0.01m RMSE compare to expert-tuned baselines?

5. **Practical Deployment:**
   - Is the total training time (6.1 hours for 4 platforms) acceptable for real-world deployment?
   - Can the framework be used by non-experts who lack MPC tuning experience?
   - Does the approach generalize to unseen drone configurations without retraining?

6. **Algorithm Selection for MPC Tuning:**
   - Which RL algorithm (PPO, TRPO, SAC, TD3, A2C) works best for MPC hyperparameter optimization?
   - Is sample efficiency or wall-clock time more important for this application?
   - Do off-policy methods (SAC, TD3) outperform on-policy methods (PPO, TRPO) for transfer learning?

**Core Hypothesis:**
RL-enhanced MPC tuning with Bryson's Rule initialization and sequential transfer learning provides:
- **Better performance** than expert manual tuning (1.34m vs. 1.5-2.0m RMSE)
- **Faster deployment** across multiple platforms (6.1 hours total vs. 16-32 hours expert time)
- **More consistent results** across heterogeneous platforms (Â±0.01m variation vs. platform-specific tuning)
- **No expert knowledge required** - automated optimization accessible to non-specialists

---

## 2. Software Requirements

### 2.1 Core Simulation & Control

| Software | Version | Purpose | Priority |
|----------|---------|---------|----------|
| Webots | R2023b+ | Primary simulation environment | Critical |
| Python | 3.8-3.11 | Programming language | Critical |
| CasADi | 3.6+ | MPC optimization backend | Critical |
| NumPy | 1.23+ | Numerical computations | Critical |
| SciPy | 1.10+ | Scientific computing | High |
| Matplotlib | 3.7+ | Visualization | High |
| PyBullet | 3.2+ | Optional parallel testing | Medium |

**Installation Commands:**
```bash
pip install numpy scipy matplotlib
pip install casadi
pip install pybullet
```

### 2.2 Reinforcement Learning Frameworks

| Framework | Version | Algorithms Supported | Priority |
|-----------|---------|---------------------|----------|
| Stable-Baselines3 | 2.0+ | PPO, SAC, TD3, A2C | Critical |
| Gymnasium | 0.29+ | Environment interface | Critical |
| PyTorch | 2.0+ | Neural network backend | Critical |
| TensorFlow | 2.13+ | Alternative backend | Optional |
| Ray RLlib | 2.7+ | Advanced algorithms | Medium |

**Installation Commands:**
```bash
pip install stable-baselines3[extra]
pip install gymnasium
pip install torch torchvision
pip install "ray[rllib]"
```

### 2.3 Bayesian Optimization Tools

| Tool | Version | Purpose | Priority |
|------|---------|---------|----------|
| BoTorch | 0.9+ | Bayesian optimization | High |
| GPyOpt | 1.2+ | Alternative BO framework | Medium |
| Optuna | 3.3+ | Hyperparameter optimization | Medium |
| Scikit-optimize | 0.9+ | Simple BO interface | Low |

**Installation Commands:**
```bash
pip install botorch
pip install gpyopt
pip install optuna
pip install scikit-optimize
```

### 2.4 Data Analysis & Visualization

| Tool | Version | Purpose | Priority |
|------|---------|---------|----------|
| Pandas | 2.0+ | Data manipulation | High |
| Seaborn | 0.12+ | Statistical visualization | High |
| Plotly | 5.16+ | Interactive plots | Medium |
| Weights & Biases | 0.15+ | Experiment tracking | High |
| TensorBoard | 2.13+ | Alternative tracking | Medium |

**Installation Commands:**
```bash
pip install pandas seaborn plotly
pip install wandb
pip install tensorboard
```

### 2.5 Development Tools

| Tool | Purpose | Priority |
|------|---------|----------|
| Git | Version control | Critical |
| Jupyter | Interactive development | High |
| pytest | Unit testing | High |
| Black | Code formatting | Medium |
| pylint | Code quality | Medium |

---

## 3. Hardware Requirements

### 3.1 Minimum Requirements
- **CPU:** Intel i5 8th gen / AMD Ryzen 5 3600 (4 cores, 8 threads)
- **RAM:** 16 GB DDR4
- **GPU:** NVIDIA GTX 1660 (6GB VRAM) with CUDA 11.0+
- **Storage:** 50 GB available SSD space
- **OS:** Windows 10/11, Ubuntu 20.04+, or macOS 12+

### 3.2 Recommended Requirements
- **CPU:** Intel i7 12th gen / AMD Ryzen 7 5800X (8+ cores)
- **RAM:** 32 GB DDR4/DDR5
- **GPU:** NVIDIA RTX 3070/4070 (8GB+ VRAM) with CUDA 12.0+
- **Storage:** 100 GB NVMe SSD
- **Network:** Stable internet for cloud logging (if using W&B)

### 3.3 Optimal Requirements (for extensive experiments)
- **CPU:** Intel i9 / AMD Ryzen 9 / Threadripper (16+ cores)
- **RAM:** 64 GB DDR5
- **GPU:** NVIDIA RTX 4090 / A6000 (24GB VRAM)
- **Storage:** 500 GB NVMe SSD
- **Backup:** Cloud storage for experiment data

### 3.4 GPU Setup
```bash
# Verify CUDA installation
nvidia-smi

# Install PyTorch with CUDA support
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# Verify GPU availability
python -c "import torch; print(torch.cuda.is_available())"
```

---

## 4. Theoretical & Knowledge Requirements

### 4.1 Model Predictive Control (MPC)

#### Core Concepts
1. **State-Space Representation**
   - Quadrotor dynamics modeling (12-dimensional state space)
   - 4-dimensional control input: [T, Ï†Ì‡_cmd, Î¸Ì‡_cmd, ÏˆÌ‡_cmd]
   - Linearization around operating points
   - Discrete-time state equations via RK4 integration (Î”t = 0.02s)

2. **Optimization Framework**
   - Nonlinear Programming (NLP) using CasADi
   - Interior Point Optimization (IPOPT solver)
   - Prediction horizon N = 10
   - Control horizon Nc = 5
   - Computational time: ~34ms per solve

3. **Cost Function Design**
   - Stage costs: ||x - x_ref||Â²_Q + ||u||Â²_R
   - Terminal costs: ||x_N - x_ref||Â²_Qf
   - Weight matrix selection (Q âˆˆ â„^12Ã—12, R âˆˆ â„^4Ã—4)
   - Penalty methods for soft constraints

4. **Bryson's Rule for Weight Initialization**
   
   **Theoretical Foundation:**
   Bryson's Rule provides a systematic method for initial weight matrix selection based on maximum acceptable deviations:
   
   ```
   Q_ii = 1 / (max acceptable deviation in x_i)Â²
   R_jj = 1 / (max acceptable deviation in u_j)Â²
   ```
   
   **Implementation for UAV Systems:**
   ```python
   # Position weights (max acceptable error: 0.5m)
   Q_position = 1 / (0.5)**2 = 4.0
   
   # Velocity weights (max acceptable error: 1.0 m/s)
   Q_velocity = 1 / (1.0)**2 = 1.0
   
   # Orientation weights (max acceptable error: 15Â° = 0.262 rad)
   Q_orientation = 1 / (0.262)**2 â‰ˆ 14.6
   
   # Angular velocity weights (max acceptable error: 0.5 rad/s)
   Q_angular = 1 / (0.5)**2 = 4.0
   
   # Control effort weights (max acceptable thrust variation: 5.0 N)
   R_control = 1 / (5.0)**2 = 0.04
   ```
   
   **Platform-Specific Scaling:**
   Bryson's Rule provides baseline values that must be scaled by platform mass and inertia:
   
   ```python
   def bryson_initialization(platform_params):
       m = platform_params['mass']
       J = platform_params['inertia']
       
       # Base Bryson's Rule values
       Q_base = np.array([4.0, 4.0, 4.0,      # position
                          1.0, 1.0, 1.0,       # velocity
                          14.6, 14.6, 14.6,    # orientation
                          4.0, 4.0, 4.0])      # angular velocity
       
       # Platform-specific scaling
       mass_scale = m / 1.0  # Normalize to 1kg reference
       inertia_scale = np.mean(np.diag(J)) / 0.01  # Normalize to reference inertia
       
       Q_scaled = Q_base * np.array([mass_scale] * 3 +      # position scaling
                                     [mass_scale] * 3 +      # velocity scaling
                                     [inertia_scale] * 3 +   # orientation scaling
                                     [inertia_scale] * 3)    # angular velocity scaling
       
       R_scaled = np.array([0.04] * 4) / mass_scale  # Inverse mass scaling for control
       
       return Q_scaled, R_scaled
   ```
   
   **Advantages for RL-Based Tuning:**
   - Provides physically meaningful initial hyperparameters
   - Reduces RL exploration space by starting in reasonable region
   - Expected to reduce training time by 20-40% vs random initialization
   - Maintains interpretability of learned weights

5. **Tunable Parameters (17-Dimensional Action Space)**
   - **Q Matrix:** State error weights (12D for position, velocity, orientation, angular velocity)
   - **R Matrix:** Control effort weights (4D for thrust and angular rate commands)
   - **Prediction Horizon (N):** Typically 8-15 steps (1D)
   - Continuous action space: a_t âˆˆ [-1, 1]^17
   - Exponential scaling for Q, R weights; linear for horizon

#### Mathematical Formulation
```
Minimize: Î£_{k=0}^{N-1} [||x_k - x_ref||Â²_Q + ||u_k||Â²_R] + ||x_N - x_ref||Â²_Qf

Subject to:
  x_{k+1} = f_d(x_k, u_k, Î¸)
  x_0 = x(t)
  u_min â‰¤ u_k â‰¤ u_max
  x_min â‰¤ x_k â‰¤ x_max
```

### 4.2 Reinforcement Learning Algorithms

#### Overview of Algorithm Categories

**Policy Gradient Methods:**
- Learn policy directly through gradient ascent on expected return
- On-policy learning (typically)
- Good for continuous action spaces
- Algorithms: PPO, TRPO

**Actor-Critic Methods:**
- Combine value function (critic) with policy (actor)
- Can be on-policy or off-policy
- Better sample efficiency than pure policy gradient
- Algorithms: SAC, TD3, A2C

#### 1. Proximal Policy Optimization (PPO) [Primary Algorithm]

**Theoretical Foundation:**
PPO uses a clipped surrogate objective to ensure stable policy updates:

```
L^CLIP(Î¸) = E_t[min(r_t(Î¸)Ã‚_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)Ã‚_t)]

where:
  r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)  (probability ratio)
  Ã‚_t = advantage estimate
  Îµ = clip range (typically 0.2)
```

**Implementation Configuration:**
```python
ppo_config = {
    # Core hyperparameters
    'learning_rate': 3e-4,
    'n_steps': 2048,        # Steps per environment per update
    'batch_size': 64,
    'n_epochs': 10,         # Optimization epochs per update
    'gamma': 0.99,          # Discount factor
    'gae_lambda': 0.95,     # GAE parameter
    'clip_range': 0.2,      # PPO clipping parameter
    
    # Network architecture
    'policy_network': [256, 256],  # Two hidden layers
    'activation': 'tanh',
    'ortho_init': True,
    
    # Regularization
    'ent_coef': 0.01,       # Entropy coefficient
    'vf_coef': 0.5,         # Value function coefficient
    'max_grad_norm': 0.5,
    
    # Training
    'n_parallel_envs': 4,
    'total_timesteps_base': 20000,
    'total_timesteps_finetune': 5000,
    
    # Transfer learning
    'transfer_lr_multiplier': 0.1,  # Reduce LR for fine-tuning
}
```

**Advantages for MPC Tuning:**
- Stable training with large policy updates
- Good balance of sample efficiency and wall-clock time
- Proven performance in published research (1.34Â±0.01m RMSE)
- Works well with 4 parallel environments

**Expected Performance:**
- Base training: 20,000 steps, ~200 minutes
- Fine-tuning: 5,000 steps, ~52-59 minutes per platform
- Convergence: Typically within 15,000-18,000 steps

#### 2. Trust Region Policy Optimization (TRPO)

**Theoretical Foundation:**
TRPO uses a trust region constraint to limit policy updates:

```
maximize E[Ï€_Î¸(a|s)/Ï€_Î¸_old(a|s) * A(s,a)]
subject to: E[KL(Ï€_Î¸_old || Ï€_Î¸)] â‰¤ Î´

where Î´ is the trust region size (typically 0.01)
```

**Implementation Configuration:**
```python
trpo_config = {
    'learning_rate': 1e-3,
    'n_steps': 2048,
    'batch_size': 128,
    'gamma': 0.99,
    'gae_lambda': 0.95,
    'max_kl': 0.01,         # Trust region size
    'cg_iters': 10,         # Conjugate gradient iterations
    'cg_damping': 0.1,
    'vf_iters': 5,          # Value function iterations
    'vf_stepsize': 1e-3,
    'n_parallel_envs': 4,
    'total_timesteps_base': 20000,
    'total_timesteps_finetune': 5000,
}
```

**Advantages:**
- Theoretical monotonic improvement guarantee
- More stable than vanilla policy gradient
- Good for sensitive control tasks

**Disadvantages:**
- Computationally expensive (conjugate gradient)
- Slower wall-clock time than PPO
- More hyperparameters to tune

**Expected Performance:**
- Training time: 1.5-2Ã— longer than PPO
- Sample efficiency: Similar to PPO
- Convergence: More stable but slower

#### 3. Soft Actor-Critic (SAC) [Off-Policy]

**Theoretical Foundation:**
SAC maximizes expected return plus entropy:

```
J(Ï€) = Î£ E_(s_t,a_t)~Ï_Ï€ [r(s_t,a_t) + Î±H(Ï€(Â·|s_t))]

where:
  H(Ï€(Â·|s_t)) = -log Ï€(a_t|s_t)  (policy entropy)
  Î± = temperature parameter (auto-tuned)
```

**Implementation Configuration:**
```python
sac_config = {
    # Core hyperparameters
    'learning_rate': 3e-4,
    'buffer_size': 100000,
    'batch_size': 256,
    'gamma': 0.99,
    'tau': 0.005,           # Soft update coefficient
    
    # SAC-specific
    'ent_coef': 'auto',     # Automatic entropy tuning
    'target_entropy': -4,   # Dimension of action space (negative)
    'ent_coef_learning_rate': 3e-4,
    
    # Training
    'train_freq': 1,
    'gradient_steps': 1,
    'learning_starts': 1000,
    'n_parallel_envs': 1,   # SAC typically uses 1 env
    'total_timesteps_base': 40000,  # Needs more steps (off-policy)
    'total_timesteps_finetune': 10000,
    
    # Network architecture
    'policy_network': [256, 256],
    'qf_network': [256, 256],
    'activation': 'relu',
}
```

**Advantages:**
- Excellent sample efficiency (off-policy)
- Automatic entropy tuning
- Good exploration through maximum entropy
- Can reuse old experience (replay buffer)

**Disadvantages:**
- More memory intensive (replay buffer)
- Longer wall-clock time per step
- Less parallelizable than PPO

**Expected Performance:**
- Sample efficiency: Better than PPO (fewer steps to convergence)
- Wall-clock time: Slower than PPO
- Best for: Complex continuous control, data efficiency

#### 4. Twin Delayed DDPG (TD3) [Off-Policy]

**Theoretical Foundation:**
TD3 addresses overestimation bias in actor-critic methods:

```
Key innovations:
1. Twin Q-networks: Use min(Q1, Q2) to reduce overestimation
2. Delayed policy updates: Update actor less frequently than critic
3. Target policy smoothing: Add noise to target actions
```

**Implementation Configuration:**
```python
td3_config = {
    # Core hyperparameters
    'learning_rate': 3e-4,
    'buffer_size': 100000,
    'batch_size': 256,
    'gamma': 0.99,
    'tau': 0.005,
    
    # TD3-specific
    'policy_delay': 2,              # Update actor every 2 critic updates
    'target_policy_noise': 0.2,     # Noise added to target policy
    'target_noise_clip': 0.5,       # Noise clipping range
    'exploration_noise': 0.1,       # Exploration noise (training)
    
    # Training
    'train_freq': 1,
    'gradient_steps': 1,
    'learning_starts': 1000,
    'n_parallel_envs': 1,
    'total_timesteps_base': 40000,
    'total_timesteps_finetune': 10000,
    
    # Network architecture
    'policy_network': [256, 256],
    'qf_network': [256, 256],
    'activation': 'relu',
}
```

**Advantages:**
- Addresses Q-value overestimation
- More stable than DDPG
- Good for continuous control

**Disadvantages:**
- Requires careful noise tuning
- Slower than PPO in wall-clock time

**Expected Performance:**
- Sample efficiency: Similar to SAC
- Stability: Better than DDPG, comparable to SAC
- Best for: Deterministic policies, stability-critical tasks

#### 5. Advantage Actor-Critic (A2C) [Synchronous]

**Theoretical Foundation:**
A2C uses advantage function to reduce variance:

```
âˆ‡_Î¸J(Î¸) = E[âˆ‡_Î¸ log Ï€_Î¸(a|s) A(s,a)]

where:
  A(s,a) = Q(s,a) - V(s)  (advantage function)
  V(s) = E[Q(s,a)]        (state value function)
```

**Implementation Configuration:**
```python
a2c_config = {
    # Core hyperparameters
    'learning_rate': 7e-4,
    'n_steps': 5,           # Shorter rollouts than PPO
    'gamma': 0.99,
    'gae_lambda': 1.0,
    
    # Regularization
    'ent_coef': 0.01,
    'vf_coef': 0.5,
    'max_grad_norm': 0.5,
    'rms_prop_eps': 1e-5,
    
    # Training
    'n_parallel_envs': 4,   # Synchronous parallel envs
    'total_timesteps_base': 30000,  # Needs more steps than PPO
    'total_timesteps_finetune': 7500,
    
    # Network architecture
    'policy_network': [256, 256],
    'activation': 'tanh',
}
```

**Advantages:**
- Simpler than PPO
- Synchronous (reproducible)
- Less memory intensive

**Disadvantages:**
- Less sample efficient than PPO
- Can be unstable with large learning rates
- Requires more careful tuning

**Expected Performance:**
- Convergence: Faster iterations but more steps needed
- Stability: Less stable than PPO
- Best for: Baseline comparison, simple tasks

#### Algorithm Comparison Summary

| Algorithm | Type | Sample Efficiency | Wall-Clock Time | Stability | Parallelization | Best Use Case |
|-----------|------|-------------------|-----------------|-----------|-----------------|---------------|
| **PPO** | On-policy | Medium | Fast | High | Excellent (4 envs) | **Primary choice, proven performance** |
| **TRPO** | On-policy | Medium | Slow | Very High | Good | Sensitive control tasks |
| **SAC** | Off-policy | High | Medium | High | Limited (1 env) | Sample-efficient learning |
| **TD3** | Off-policy | High | Medium | High | Limited (1 env) | Deterministic control |
| **A2C** | On-policy | Low | Fast | Medium | Excellent (4 envs) | Baseline comparison |

#### RL Formulation for MPC Tuning

**State Space (29-dimensional):**
```python
s_t = [
    e_pos (3),           # Position error
    e_vel (3),           # Velocity error  
    u_eff (4),           # Recent control effort
    q_curr (12),         # Current Q weights
    r_curr (4),          # Current R weights
    N_curr (1),          # Current horizon
    t_settle (1),        # Settling time
    overshoot (1)        # Overshoot metric
]
```

**Action Space (17-dimensional):**
```python
a_t = [
    Î±_Q_pos (3),         # Position weight adjustments
    Î±_Q_vel (3),         # Velocity weight adjustments
    Î±_Q_att (3),         # Orientation weight adjustments
    Î±_Q_omega (3),       # Angular velocity weight adjustments
    Î±_R (4),             # Control effort weight adjustments
    Î”N (1)               # Horizon adjustment
] âˆˆ [-1, 1]^17
```

**Reward Function (Proven in Research):**
```python
r_t = -10.0 * ||e_pos||_2 - 1.0 * ||e_vel||_2 - 0.01 * ||u_t||_2 - 5.0 * ðŸ™[overshoot > 0.5]

where:
  Î»_pos = 10.0   (position tracking priority)
  Î»_vel = 1.0    (velocity tracking)
  Î»_control = 0.01  (control effort penalty)
  Î»_overshoot = 5.0  (stability penalty)
```

### 4.3 Bayesian Optimization

#### Key Components
1. **Surrogate Model:** Gaussian Process (GP) regression
2. **Acquisition Functions:**
   - Expected Improvement (EI)
   - Upper Confidence Bound (UCB)
   - Probability of Improvement (PI)
3. **Search Space:** MPC parameter bounds
4. **Objective Function:** Cumulative trajectory tracking error

### 4.4 Transfer Learning

#### Concepts
1. **Domain Adaptation:** Light â†’ Heavy drone transfer
2. **Fine-tuning Strategies:** 
   - Full parameter fine-tuning
   - Partial layer freezing
   - Progressive unfreezing
3. **Meta-Learning:** Learning to adapt quickly (optional advanced topic)

---

## 5. Drone Configuration Requirements

### 5.1 Platform Overview (From Published Research)

Our framework is validated across four distinct UAV platforms representing 200Ã— mass variation and 6000Ã— inertia variation:

| Platform | Mass (kg) | Type | Ixx (kgÂ·mÂ²) | Application |
|----------|-----------|------|-------------|-------------|
| Crazyflie 2.X | 0.027 | Micro quadrotor | 1.4Ã—10â»âµ | Indoor inspection, research |
| Racing Quadrotor | 0.800 | Racing drone | 8.1Ã—10â»Â³ | Fast response, agile maneuvers |
| Generic Quadrotor | 2.500 | Standard drone | 2.8Ã—10â»Â² | Payload delivery, photography |
| Heavy-Lift Hexacopter | 5.500 | Industrial hex | 8.4Ã—10â»Â² | Heavy payloads, industrial |

### 5.2 Crazyflie 2.X (Base Platform)

#### Physical Parameters
```yaml
crazyflie_2x:
  name: "Crazyflie 2.X Nano Quadrotor"
  mass: 0.027  # kg
  dimensions:
    arm_length: 0.046  # m (46mm rotor-to-rotor: 92mm)
    rotor_diameter: 0.046  # m
    body_height: 0.029  # m
  
  inertia_matrix:  # kgÂ·mÂ²
    Ixx: 1.395e-5
    Iyy: 1.436e-5
    Izz: 2.173e-5
  
  motor_specifications:
    type: "Coreless DC motors"
    max_thrust_per_motor: 0.15  # N (total: 0.6N, hover at ~40%)
    time_constant: 0.015  # s
    max_rpm: 21000
    propeller_constant: 2.3e-6  # N/(rad/s)Â²
  
  aerodynamics:
    drag_coefficient_xy: 0.01
    drag_coefficient_z: 0.02
  
  battery:
    capacity: 240  # mAh
    voltage: 3.7  # V
    flight_time: 7  # minutes (typical)
  
  control_constraints:
    max_roll_pitch: 30  # degrees
    max_yaw_rate: 200  # deg/s
    max_vertical_speed: 1.5  # m/s
```

#### Bryson's Rule Initialization (Crazyflie)
```python
# Maximum acceptable deviations for nano drone
Q_crazyflie = np.diag([
    100.0, 100.0, 100.0,  # position (0.1m acceptable error)
    16.0, 16.0, 16.0,     # velocity (0.25 m/s acceptable)
    25.0, 25.0, 10.0,     # orientation (0.2 rad â‰ˆ 11.5Â°)
    16.0, 16.0, 16.0      # angular velocity (0.25 rad/s)
])

R_crazyflie = np.diag([
    0.25, 0.25, 0.25, 0.25  # control effort (0.02N acceptable per motor)
])
```

### 5.3 Racing Quadrotor (Transfer Platform 1)

#### Physical Parameters
```yaml
racing_quadrotor:
  name: "High-Performance Racing Drone"
  mass: 0.800  # kg (30Ã— heavier than Crazyflie)
  dimensions:
    arm_length: 0.150  # m
    rotor_diameter: 0.127  # m (5-inch props)
    body_height: 0.050  # m
  
  inertia_matrix:  # kgÂ·mÂ²
    Ixx: 8.1e-3
    Iyy: 8.1e-3
    Izz: 1.4e-2
  
  motor_specifications:
    type: "Brushless outrunner (2300KV)"
    max_thrust_per_motor: 7.5  # N (total: 30N, 3.75:1 thrust ratio)
    time_constant: 0.020  # s
    max_rpm: 30000
    propeller_constant: 4.5e-5  # N/(rad/s)Â²
  
  aerodynamics:
    drag_coefficient_xy: 0.015
    drag_coefficient_z: 0.025
  
  battery:
    capacity: 1300  # mAh
    voltage: 14.8  # V (4S LiPo)
    flight_time: 5  # minutes (racing configuration)
  
  control_constraints:
    max_roll_pitch: 60  # degrees (aggressive)
    max_yaw_rate: 500  # deg/s
    max_vertical_speed: 10  # m/s
```

#### Bryson's Rule Initialization (Racing)
```python
Q_racing = np.diag([
    64.0, 64.0, 64.0,     # position (0.125m acceptable)
    25.0, 25.0, 25.0,     # velocity (0.2 m/s)
    36.0, 36.0, 16.0,     # orientation (0.167 rad â‰ˆ 9.6Â°)
    25.0, 25.0, 25.0      # angular velocity (0.2 rad/s)
])

R_racing = np.diag([
    0.04, 0.04, 0.04, 0.04  # control effort (scaled by mass)
])
```

### 5.4 Generic Quadrotor (Transfer Platform 2)

#### Physical Parameters
```yaml
generic_quadrotor:
  name: "Standard Commercial Quadrotor"
  mass: 2.500  # kg (93Ã— heavier than Crazyflie)
  dimensions:
    arm_length: 0.250  # m
    rotor_diameter: 0.254  # m (10-inch props)
    body_height: 0.100  # m
  
  inertia_matrix:  # kgÂ·mÂ²
    Ixx: 2.8e-2
    Iyy: 2.8e-2
    Izz: 4.8e-2
  
  motor_specifications:
    type: "Brushless outrunner (920KV)"
    max_thrust_per_motor: 12.5  # N (total: 50N, 2:1 thrust ratio)
    time_constant: 0.025  # s
    max_rpm: 15000
    propeller_constant: 1.2e-4  # N/(rad/s)Â²
  
  aerodynamics:
    drag_coefficient_xy: 0.020
    drag_coefficient_z: 0.030
  
  battery:
    capacity: 5000  # mAh
    voltage: 22.2  # V (6S LiPo)
    flight_time: 20  # minutes
  
  payload:
    max_payload: 1.0  # kg (camera, sensors)
  
  control_constraints:
    max_roll_pitch: 35  # degrees
    max_yaw_rate: 150  # deg/s
    max_vertical_speed: 5  # m/s
```

#### Bryson's Rule Initialization (Generic)
```python
Q_generic = np.diag([
    16.0, 16.0, 16.0,     # position (0.25m acceptable)
    4.0, 4.0, 4.0,        # velocity (0.5 m/s)
    16.0, 16.0, 6.25,     # orientation (0.25 rad â‰ˆ 14.3Â°)
    4.0, 4.0, 4.0         # angular velocity (0.5 rad/s)
])

R_generic = np.diag([
    0.016, 0.016, 0.016, 0.016
])
```

### 5.5 Heavy-Lift Hexacopter (Transfer Platform 3)

#### Physical Parameters
```yaml
heavy_lift_hexacopter:
  name: "Industrial Heavy-Lift Hexacopter"
  mass: 5.500  # kg (204Ã— heavier than Crazyflie)
  dimensions:
    arm_length: 0.400  # m
    rotor_diameter: 0.381  # m (15-inch props)
    body_height: 0.150  # m
    num_rotors: 6
  
  inertia_matrix:  # kgÂ·mÂ²
    Ixx: 8.4e-2
    Iyy: 8.4e-2
    Izz: 1.5e-1
  
  motor_specifications:
    type: "High-torque brushless (400KV)"
    max_thrust_per_motor: 18.0  # N (total: 108N, 2:1 thrust ratio)
    time_constant: 0.030  # s
    max_rpm: 8000
    propeller_constant: 3.5e-4  # N/(rad/s)Â²
  
  aerodynamics:
    drag_coefficient_xy: 0.030
    drag_coefficient_z: 0.040
  
  battery:
    capacity: 16000  # mAh
    voltage: 44.4  # V (12S LiPo)
    flight_time: 25  # minutes (with payload)
  
  payload:
    max_payload: 5.0  # kg (industrial equipment)
  
  control_constraints:
    max_roll_pitch: 25  # degrees (conservative for safety)
    max_yaw_rate: 100  # deg/s
    max_vertical_speed: 4  # m/s
```

#### Bryson's Rule Initialization (Heavy-Lift)
```python
Q_heavy = np.diag([
    4.0, 4.0, 4.0,        # position (0.5m acceptable)
    1.0, 1.0, 1.0,        # velocity (1.0 m/s)
    6.25, 6.25, 2.78,     # orientation (0.4 rad â‰ˆ 22.9Â°)
    1.0, 1.0, 1.0         # angular velocity (1.0 rad/s)
])

R_heavy = np.diag([
    0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064  # 6 motors
])
```

### 5.6 Sensor Suite (All Platforms)

All platforms include identical sensor configurations for consistency:

```yaml
sensors:
  imu:
    accelerometer:
      range: Â±16g
      noise_density: 0.01 m/sÂ²
      bias_instability: 0.001 m/sÂ²
      update_rate: 1000 Hz
    
    gyroscope:
      range: Â±2000 deg/s
      noise_density: 0.001 rad/s
      bias_instability: 0.0001 rad/s
      update_rate: 1000 Hz
  
  gps:
    position_accuracy: 0.1 m (CEP)
    velocity_accuracy: 0.05 m/s
    update_rate: 10 Hz
    time_to_first_fix: 2 s
  
  barometer:
    altitude_accuracy: 0.05 m
    resolution: 0.01 m
    update_rate: 50 Hz
  
  magnetometer:
    heading_accuracy: 1.0 deg
    update_rate: 75 Hz
```

### 5.7 Platform Comparison Matrix

| Parameter | Crazyflie | Racing | Generic | Heavy-Lift | Variation |
|-----------|-----------|--------|---------|------------|-----------|
| Mass (kg) | 0.027 | 0.800 | 2.500 | 5.500 | 204Ã— |
| Ixx (kgÂ·mÂ²) | 1.4Ã—10â»âµ | 8.1Ã—10â»Â³ | 2.8Ã—10â»Â² | 8.4Ã—10â»Â² | 6000Ã— |
| Max Thrust (N) | 0.60 | 30.0 | 50.0 | 108.0 | 180Ã— |
| Thrust/Weight | 2.26 | 3.75 | 2.04 | 2.00 | - |
| Max Speed (m/s) | 1.5 | 10.0 | 5.0 | 4.0 | - |
| Num Rotors | 4 | 4 | 4 | 6 | - |

---

## 6. Webots Simulation Requirements

### 6.1 World Setup

#### Environment Components
1. **Coordinate System:** ENU (East-North-Up)
2. **Gravity:** 9.81 m/sÂ²
3. **Arena Size:** 20m x 20m x 10m (minimum)
4. **Obstacles:** Optional for advanced scenarios
5. **Wind Model:** Gaussian wind disturbances (optional)

#### Lighting & Physics
```
WorldInfo:
  basicTimeStep: 10 ms
  FPS: 30
  coordinateSystem: ENU
  
Physics:
  gravity: [0, 0, -9.81]
  air_density: 1.225 kg/mÂ³
```

### 6.2 Drone PROTO Files

#### Required PROTO Structures
1. **LightDrone.proto** - Lightweight quadrotor model
2. **HeavyDrone.proto** - Heavyweight quadrotor model
3. **MediumDrone.proto** - Intermediate configuration

#### PROTO Components
```
PROTO DroneTemplate [
  field SFVec3f translation
  field SFRotation rotation
  field SFFloat mass
  field SFVec3f inertia
  field MFNode rotors
  field MFNode sensors
]
```

### 6.3 Controller Structure

#### Supervisor Controller
```python
# webots_controllers/supervisor_controller.py
- Initialize simulation environment
- Reset drone positions
- Monitor performance metrics
- Log training data
- Handle episode termination
```

#### MPC Controller
```python
# webots_controllers/mpc_controller.py
- Receive sensor data
- Compute optimal control
- Send motor commands
- Update state estimates
```

#### RL Agent Controller
```python
# webots_controllers/rl_agent_controller.py
- Interface with Gymnasium environment
- Execute learned policies
- Collect experience
- Trigger parameter updates
```

### 6.4 Webots-Python Interface

```python
from controller import Robot, Supervisor, GPS, InertialUnit, Motor

class DroneController(Supervisor):
    def __init__(self):
        super().__init__()
        self.timestep = int(self.getBasicTimeStep())
        self.initialize_devices()
    
    def initialize_devices(self):
        # GPS, IMU, Motors, etc.
        pass
```

---

## 7. MPC Implementation Requirements

### 7.1 State-Space Model

#### Continuous-Time Dynamics
```
áº‹ = f(x, u) where:
  x = [px, py, pz, vx, vy, vz, Ï†, Î¸, Ïˆ, p, q, r]áµ€  (12 states)
  u = [T1, T2, T3, T4]áµ€                            (4 inputs)
```

#### Discretization
- Method: Zero-Order Hold (ZOH) or Runge-Kutta 4
- Sampling time: 0.02-0.05 seconds

### 7.2 Cost Function Structure

#### Standard Form
```
J = Î£_{k=0}^{N-1} [(x_k - x_ref)áµ€Q(x_k - x_ref) + u_káµ€Ru_k + Î”u_káµ€SÎ”u_k] 
    + (x_N - x_ref)áµ€P(x_N - x_ref)
```

#### Weight Matrices (Initial Values)
```python
Q_initial = np.diag([
    100, 100, 100,  # position errors (x, y, z)
    10, 10, 10,     # velocity errors
    50, 50, 20,     # orientation errors (roll, pitch, yaw)
    1, 1, 1         # angular velocity errors
])

R_initial = np.diag([1, 1, 1, 1])  # control effort

S_initial = np.diag([0.1, 0.1, 0.1, 0.1])  # control rate
```

### 7.3 Constraints

#### Input Constraints
```python
u_min = [0, 0, 0, 0]  # Minimum thrust per motor
u_max = [max_thrust, max_thrust, max_thrust, max_thrust]
```

#### State Constraints (Safety)
```python
roll_max = 30 deg
pitch_max = 30 deg
velocity_max = 5 m/s
altitude_min = 0.5 m
altitude_max = 8 m
```

### 7.4 Tunable Parameters

| Parameter | Symbol | Range | Type | Priority |
|-----------|--------|-------|------|----------|
| Position weight | Q_pos | [10, 500] | Continuous | High |
| Velocity weight | Q_vel | [1, 100] | Continuous | High |
| Orientation weight | Q_att | [10, 200] | Continuous | High |
| Control effort | R | [0.1, 10] | Continuous | High |
| Prediction horizon | Np | [10, 50] | Discrete | Medium |
| Control horizon | Nc | [5, 20] | Discrete | Medium |
| Terminal weight | P | [50, 500] | Continuous | Low |

### 7.5 MPC Solver Configuration

```python
mpc_options = {
    'solver': 'ipopt',  # or 'qpoases', 'osqp'
    'max_iter': 100,
    'tolerance': 1e-6,
    'warm_start': True,
    'print_level': 0
}
```

---

## 8. RL Environment Requirements

### 8.1 Gymnasium Environment Specification

#### State Space (Observation)
```python
observation_space = spaces.Box(
    low=-np.inf, 
    high=np.inf, 
    shape=(N_obs,),
    dtype=np.float32
)

# Components (N_obs = 30-40):
# - Drone state: [position, velocity, orientation, angular_velocity] (12)
# - Reference trajectory: [ref_position, ref_velocity] (6)
# - Tracking error: [position_error, velocity_error] (6)
# - MPC status: [current_Q_weights, current_R_weights] (16)
# - Performance metrics: [cumulative_error, control_variance] (2-4)
```

#### Action Space
```python
# Option 1: Direct weight modification (Continuous)
action_space = spaces.Box(
    low=-1.0,
    high=1.0,
    shape=(16,),  # 12 Q weights + 4 R weights
    dtype=np.float32
)

# Option 2: Multiplicative factors (Continuous, more stable)
action_space = spaces.Box(
    low=0.5,   # Scale weights by 0.5x to 2.0x
    high=2.0,
    shape=(16,),
    dtype=np.float32
)

# Option 3: Discrete adjustments
action_space = spaces.MultiDiscrete([5, 5, 5, ...])  # 5 levels per weight
```

#### Reward Function Design

```python
def compute_reward(state, action, next_state):
    # Primary objective: Minimize tracking error
    position_error = np.linalg.norm(state['position'] - state['reference_position'])
    velocity_error = np.linalg.norm(state['velocity'] - state['reference_velocity'])
    
    tracking_reward = -1.0 * (position_error + 0.5 * velocity_error)
    
    # Secondary: Control smoothness
    control_effort = np.sum(state['control_input']**2)
    control_penalty = -0.1 * control_effort
    
    # Tertiary: Stability
    orientation_error = np.linalg.norm(state['orientation'])
    stability_penalty = -0.5 * orientation_error if orientation_error > 0.5 else 0
    
    # Bonus for good convergence
    convergence_bonus = 10.0 if position_error < 0.1 else 0
    
    # Penalty for constraint violations
    constraint_penalty = -50.0 if check_constraints_violated(state) else 0
    
    total_reward = (tracking_reward + 
                   control_penalty + 
                   stability_penalty + 
                   convergence_bonus + 
                   constraint_penalty)
    
    return total_reward
```

### 8.2 Episode Configuration

```python
episode_config = {
    'max_steps': 500,  # 10 seconds at 50Hz
    'initial_position_range': [[-2, 2], [-2, 2], [1, 3]],  # x, y, z
    'reference_trajectories': [
        'hover',
        'circular',
        'lemniscate',
        'waypoint_sequence'
    ],
    'reset_on_crash': True,
    'crash_threshold': {
        'altitude': 0.2,
        'tilt_angle': 60  # degrees
    }
}
```

### 8.3 Training Configuration

#### Hyperparameters (PPO)
```python
ppo_config = {
    'learning_rate': 3e-4,
    'n_steps': 2048,
    'batch_size': 64,
    'n_epochs': 10,
    'gamma': 0.99,
    'gae_lambda': 0.95,
    'clip_range': 0.2,
    'ent_coef': 0.01,
    'vf_coef': 0.5,
    'max_grad_norm': 0.5
}
```

#### Hyperparameters (SAC)
```python
sac_config = {
    'learning_rate': 3e-4,
    'buffer_size': 100000,
    'batch_size': 256,
    'gamma': 0.99,
    'tau': 0.005,
    'ent_coef': 'auto',
    'target_update_interval': 1,
    'train_freq': 1
}
```

#### Hyperparameters (TD3)
```python
td3_config = {
    'learning_rate': 3e-4,
    'buffer_size': 100000,
    'batch_size': 256,
    'gamma': 0.99,
    'tau': 0.005,
    'policy_delay': 2,
    'target_policy_noise': 0.2,
    'target_noise_clip': 0.5
}
```

---

## 9. Bayesian Optimization Requirements

### 9.1 Search Space Definition

```python
search_space = {
    'Q_position_xy': (10.0, 500.0),
    'Q_position_z': (10.0, 500.0),
    'Q_velocity': (1.0, 100.0),
    'Q_roll': (10.0, 200.0),
    'Q_pitch': (10.0, 200.0),
    'Q_yaw': (5.0, 100.0),
    'Q_angular_vel': (1.0, 50.0),
    'R_motors': (0.1, 10.0),
    'N_prediction': (10, 50),  # discrete
    'N_control': (5, 20)       # discrete
}
```

### 9.2 Objective Function

```python
def bayesian_objective(params):
    """
    Objective function for Bayesian Optimization
    Returns: negative cumulative tracking error (to maximize)
    """
    # Configure MPC with params
    mpc_controller.set_parameters(params)
    
    # Run multiple test trajectories
    total_error = 0
    for trajectory in test_trajectories:
        simulation_result = run_simulation(mpc_controller, trajectory)
        total_error += simulation_result['cumulative_error']
    
    # Return negative (BO maximizes by default)
    return -total_error
```

### 9.3 BO Configuration

```python
bo_config = {
    'n_initial_points': 10,  # Random sampling
    'n_iterations': 50,
    'acquisition_function': 'Expected Improvement',
    'kernel': 'Matern52',
    'xi': 0.01,  # Exploration parameter
    'kappa': 2.576,  # UCB parameter
    'normalize_y': True
}
```

---

## 10. Experimental Design Requirements

### 10.1 Test Trajectories

Based on the published research, we use the following standard trajectories:

#### Trajectory 1: Hovering (Baseline Performance)
```python
hover_trajectory = {
    'type': 'hover',
    'position': [0, 0, 2],  # 2m altitude
    'duration': 10.0,  # seconds
    'disturbances': {
        'wind_gust': False,  # Clean hover for baseline
        'measurement_noise': True
    },
    'success_criteria': {
        'position_error': 0.1,  # m
        'settling_time': 3.0    # s
    }
}
```

#### Trajectory 2: Circular Path (Tracking Performance)
```python
circular_trajectory = {
    'type': 'circular',
    'center': [0, 0, 2],
    'radius': 2.0,  # m
    'angular_velocity': 0.5,  # rad/s
    'duration': 25.0,  # One complete circle ~12.5s
    'success_criteria': {
        'rmse': 1.5  # m (target from paper: 1.34m)
    }
}
```

#### Trajectory 3: Waypoint Navigation
```python
waypoint_trajectory = {
    'type': 'waypoints',
    'waypoints': [
        [0, 0, 1],    # Start
        [3, 0, 2],    # Forward
        [3, 3, 3],    # Right and up
        [-2, 3, 2],   # Back-left and down
        [0, 0, 1]     # Return home
    ],
    'velocities': [1.5, 1.5, 2.0, 1.5],  # m/s between waypoints
    'duration': 20.0,
    'success_criteria': {
        'waypoint_arrival_threshold': 0.3,  # m
        'rmse': 1.5
    }
}
```

#### Trajectory 4: Lemniscate (Figure-8) - Aggressive Maneuver
```python
lemniscate_trajectory = {
    'type': 'lemniscate',
    'scale': 2.0,      # Size of figure-8
    'height': 2.0,     # Center altitude
    'period': 10.0,    # Time for one complete figure-8
    'duration': 30.0,  # Three complete cycles
    'equation': 'x = scale * sin(t/period), y = scale * sin(2*t/period)',
    'success_criteria': {
        'rmse': 1.8  # Slightly higher tolerance for aggressive maneuver
    }
}
```

### 10.2 Performance Metrics (From Published Research)

#### Primary Metrics (Directly from Paper)
```python
primary_metrics = {
    'tracking_performance': {
        'RMSE_position': {
            'target': 1.34,          # m (achieved in paper)
            'tolerance': 0.01,       # Â±0.01m across platforms
            'measurement': 'sqrt(mean((x - x_ref)^2))'
        },
        'RMSE_velocity': {
            'target': 0.5,           # m/s
            'measurement': 'sqrt(mean((v - v_ref)^2))'
        },
        'max_error': {
            'acceptable': 2.5,       # m
            'measurement': 'max(|x - x_ref|)'
        }
    },
    
    'control_performance': {
        'control_effort': {
            'crazyflie': 0.23,       # Actual values from paper
            'racing': 0.35,
            'generic': 0.22,
            'heavy': 0.19,
            'measurement': 'mean(||u||_2)'
        },
        'control_smoothness': {
            'measurement': 'var(u)',
            'lower_is_better': True
        },
        'settling_time': {
            'target': 3.0,           # seconds
            'measurement': 'Time to reach 95% of reference'
        }
    },
    
    'stability': {
        'max_tilt_angle': {
            'limit': 30,             # degrees (safety constraint)
            'measurement': 'max(|phi|, |theta|)'
        },
        'overshoot': {
            'acceptable': 0.5,       # m
            'penalty': 5.0           # in reward function
        }
    }
}
```

#### Training Metrics (Validated Results)
```python
training_metrics = {
    'sample_efficiency': {
        'base_training_steps': 20000,        # Crazyflie
        'fine_tuning_steps': 5000,           # Subsequent platforms
        'reduction_percentage': 75.0,        # 75% fewer steps
        'measurement': 'Total timesteps to convergence'
    },
    
    'wall_clock_time': {
        'base_training_minutes': 200.3,      # Actual measured time
        'fine_tuning_minutes': {
            'racing': 52.1,
            'generic': 52.0,
            'heavy': 58.6
        },
        'total_time_minutes': 362.9,
        'total_time_hours': 6.1,
        'time_reduction_percentage': 56.2,   # vs no transfer
        'measurement': 'Actual wall-clock training time'
    },
    
    'convergence_quality': {
        'final_episode_reward': {
            'target': -5.0,          # Based on reward function design
            'measurement': 'Mean reward over last 100 episodes'
        },
        'training_stability': {
            'measurement': 'std(episodic_returns)',
            'target': '<2.0'
        }
    }
}
```

#### Transfer Learning Metrics
```python
transfer_metrics = {
    'zero_shot_performance': {
        'description': 'Performance immediately after policy transfer (no fine-tuning)',
        'expected_degradation': '20-30%',
        'measurement': 'RMSE on target platform using source policy'
    },
    
    'fine_tuning_efficiency': {
        'steps_ratio': 0.25,         # 25% of baseline (5000/20000)
        'time_ratio': 0.26,          # ~26% of baseline time
        'performance_retention': 1.00, # 100% performance maintained
        'measurement': 'Steps needed to reach baseline performance'
    },
    
    'cross_platform_consistency': {
        'rmse_std': 0.01,            # Standard deviation across platforms
        'rmse_mean': 1.34,           # Mean across platforms
        'coefficient_of_variation': 0.007,  # 0.7% variation
        'measurement': 'Consistency of RMSE across 200Ã— mass range'
    }
}
```

### 10.3 Baseline Comparisons

#### Primary Comparison: RL+Transfer vs. Expert Manual Tuning

The **key research contribution** is demonstrating that RL-enhanced MPC with transfer learning outperforms traditional expert manual tuning:

```python
comparison_framework = {
    'expert_manual_tuning': {
        'description': 'Experienced control engineer manually tunes MPC for each platform',
        'methodology': [
            '1. Start with conservative default weights',
            '2. Test on hovering task, observe performance',
            '3. Iteratively adjust Q, R weights based on tracking error and control effort',
            '4. Test on circular trajectory',
            '5. Fine-tune until satisfactory performance',
            '6. Repeat entire process for next platform'
        ],
        'time_per_platform': '4-8 hours',
        'total_time_4_platforms': '16-32 hours',
        'expected_rmse': {
            'crazyflie': '1.5-2.0m',      # Difficult due to low mass
            'racing': '1.3-1.8m',         # Easier (good thrust/weight)
            'generic': '1.4-1.9m',        # Standard performance
            'heavy': '1.6-2.2m'           # Difficult due to high inertia
        },
        'consistency': 'Platform-dependent, high variance',
        'transferability': 'None - must tune each platform separately',
        'expertise_required': 'High - requires deep MPC knowledge',
        'cost': 'Expert engineer salary Ã— 16-32 hours',
        'pros': [
            'Interpretable parameters',
            'Can incorporate domain knowledge',
            'No training infrastructure needed'
        ],
        'cons': [
            'Time-consuming and expensive',
            'Results depend on engineer experience',
            'No knowledge transfer between platforms',
            'Difficult to find globally optimal parameters',
            'Inconsistent across platforms'
        ]
    },
    
    'rl_tuned_with_transfer': {
        'description': 'RL-based tuning on base platform, then transfer to others (PROPOSED)',
        'methodology': [
            '1. Apply Bryson\'s Rule for initial weights',
            '2. Train PPO on Crazyflie (20,000 steps, 200 min)',
            '3. Transfer policy to Racing, fine-tune (5,000 steps, 52 min)',
            '4. Transfer to Generic, fine-tune (5,000 steps, 52 min)',
            '5. Transfer to Heavy-Lift, fine-tune (5,000 steps, 59 min)'
        ],
        'time_per_platform': {
            'base': '200 minutes (3.3 hours)',
            'transfer': '52-59 minutes (0.9-1.0 hours)'
        },
        'total_time_4_platforms': '363 minutes (6.1 hours)',
        'achieved_rmse': {
            'crazyflie': '1.33m',
            'racing': '1.34m',
            'generic': '1.34m',
            'heavy': '1.34m'
        },
        'consistency': 'Excellent - 1.34Â±0.01m across all platforms',
        'transferability': 'Excellent - 75% step reduction',
        'expertise_required': 'Low - automated optimization',
        'cost': 'One-time compute cost (consumer hardware)',
        'pros': [
            '62-81% time savings vs expert (6.1h vs 16-32h)',
            '11-47% better RMSE performance',
            'Consistent across 200Ã— mass variation',
            'No expert knowledge required',
            'Knowledge transfers between platforms',
            'Reproducible and automated'
        ],
        'cons': [
            'Initial setup and training infrastructure',
            'Requires understanding of RL framework',
            'Black-box policy (less interpretable)'
        ]
    },
    
    'bryson_rule_only': {
        'description': 'Analytical Bryson\'s Rule without RL optimization',
        'methodology': [
            '1. Define maximum acceptable deviations per platform',
            '2. Calculate Q = 1/(max_deviation)Â²',
            '3. Calculate R = 1/(max_control_effort)Â²',
            '4. Deploy without iteration'
        ],
        'time_per_platform': '10-15 minutes (analytical)',
        'total_time_4_platforms': '40-60 minutes',
        'expected_rmse': {
            'crazyflie': '2.0-3.0m',
            'racing': '1.8-2.5m',
            'generic': '1.9-2.6m',
            'heavy': '2.2-3.2m'
        },
        'consistency': 'Good - scales with platform physics',
        'transferability': 'Excellent - same formula applies',
        'expertise_required': 'Medium - need to define acceptable deviations',
        'cost': 'Minimal',
        'pros': [
            'Very fast deployment',
            'Highly interpretable',
            'Physically meaningful parameters',
            'Good starting point'
        ],
        'cons': [
            'Suboptimal performance (2-3m RMSE)',
            'Cannot adapt to trajectory complexity',
            'No learning or improvement over time'
        ]
    },
    
    'rl_without_transfer': {
        'description': 'Train RL from scratch on each platform independently',
        'methodology': [
            'Train each platform for 20,000 steps with Bryson\'s initialization'
        ],
        'time_per_platform': '200 minutes',
        'total_time_4_platforms': '800 minutes (13.3 hours)',
        'expected_rmse': {
            'crazyflie': '1.33m',
            'racing': '1.34m',
            'generic': '1.33m',
            'heavy': '1.35m'
        },
        'consistency': 'Excellent',
        'transferability': 'None used',
        'expertise_required': 'Low',
        'cost': 'Higher compute cost',
        'pros': [
            'Optimal per-platform performance',
            'No transfer learning complexity'
        ],
        'cons': [
            '54% longer training time (13.3h vs 6.1h)',
            'Wastes learning from previous platforms',
            'Not scalable to large fleets'
        ]
    }
}
```

#### Comparison Matrix

| Method | Time (4 platforms) | RMSE | Consistency | Expertise | Transferability | Cost |
|--------|-------------------|------|-------------|-----------|-----------------|------|
| **Expert Manual** | 16-32 hours | 1.5-2.2m | Poor | High | None | High |
| **RL+Transfer (Ours)** | **6.1 hours** | **1.34Â±0.01m** | **Excellent** | **Low** | **Excellent** | Low |
| Bryson's Rule Only | 1 hour | 2.0-3.0m | Good | Medium | Excellent | Minimal |
| RL No Transfer | 13.3 hours | 1.33-1.35m | Excellent | Low | None | Medium |

#### Key Value Propositions

**Compared to Expert Manual Tuning:**
- **62-81% time savings:** 6.1 hours vs. 16-32 hours
- **11-47% better RMSE:** 1.34m vs. 1.5-2.2m  
- **Democratizes MPC tuning:** No expert knowledge required
- **Consistent performance:** Â±0.01m vs. platform-dependent variation

**Compared to Bryson's Rule Only:**
- **33-55% better RMSE:** 1.34m vs. 2.0-3.0m
- **Optimizes beyond analytical initialization:** Learns trajectory-specific adaptations
- **Trade-off:** 6Ã— longer deployment but 2Ã— better performance

**Compared to RL without Transfer:**
- **54% time savings:** 6.1 hours vs. 13.3 hours
- **Same performance:** Both achieve ~1.34m RMSE
- **Scalability:** Transfer approach scales better for large fleets

### 10.4 Statistical Analysis Requirements

Based on published research methodology:

```python
statistical_analysis = {
    'experimental_design': {
        'independent_runs': 3,           # Minimum for reproducibility
        'recommended_runs': 5,           # For robust statistics
        'random_seeds': [42, 123, 456, 789, 2024],
        'platforms_tested': 4,
        'trajectories_per_platform': 4   # hover, circle, waypoint, lemniscate
    },
    
    'significance_testing': {
        'alpha': 0.05,                   # Significance level
        'tests': [
            'paired_t_test',             # Compare RL algorithms
            'wilcoxon_signed_rank',      # Non-parametric alternative
            'friedman_test',             # Multiple algorithm comparison
            'bonferroni_correction'      # Multiple comparison correction
        ],
        'effect_size': 'cohens_d',
        'confidence_intervals': 0.95
    },
    
    'reproducibility': {
        'seed_control': True,
        'environment_determinism': True,
        'checkpoint_frequency': 1000,    # Save every 1000 steps
        'logging_frequency': 100,        # Log every 100 steps
        'video_recording': True          # Record successful trajectories
    }
}
```

### 10.5 Computational Performance Metrics

```python
computational_metrics = {
    'mpc_solver_performance': {
        'solve_time_mean': 34,           # ms (actual measured)
        'solve_time_std': 5,             # ms
        'solve_success_rate': 0.998,     # 99.8%
        'control_frequency': 50,         # Hz (20ms timestep)
        'real_time_factor': 1.7          # Simulation runs 1.7Ã— real-time
    },
    
    'parallel_training': {
        'num_environments': 4,
        'speedup_factor': 3.7,           # vs single environment
        'cpu_utilization': 0.85,         # 85% average
        'gpu_utilization': 0.60,         # 60% average (if available)
    },
    
    'hardware_requirements': {
        'cpu': 'Intel i7-10700K or equivalent',
        'ram': '16 GB',
        'gpu': 'Optional (NVIDIA GTX 1660 or better)',
        'storage': '50 GB SSD',
        'training_time_no_gpu': '6.1 hours',
        'training_time_with_gpu': '~5.0 hours (estimated)'
    }
}
```

### 10.6 Success Criteria (Quantitative Targets)

```python
success_criteria = {
    'tracking_performance': {
        'rmse_target': 1.34,
        'rmse_tolerance': 0.01,
        'cross_platform_consistency': 'PASS if std(RMSE) â‰¤ 0.01m'
    },
    
    'training_efficiency': {
        'base_training': 'PASS if converges within 20,000 steps',
        'fine_tuning': 'PASS if converges within 5,000 steps',
        'time_reduction': 'PASS if total time < 400 minutes'
    },
    
    'transfer_learning': {
        'step_reduction': 'PASS if fine-tuning uses â‰¤30% of base steps',
        'performance_retention': 'PASS if RMSE degradation <5%',
        'generalization': 'PASS across all 4 platforms'
    },
    
    'algorithm_comparison': {
        'ppo_baseline': 'Must achieve published results',
        'other_algorithms': 'Compare sample efficiency and wall-clock time',
        'statistical_significance': 'p < 0.05 for claimed improvements'
    }
}
```

---

## 11. Transfer Learning Requirements

### 11.1 Transfer Strategy

#### Stage 1: Pre-training (Light Drone)
```python
pretrain_config = {
    'drone_mass': 1.0,
    'training_episodes': 1000,
    'convergence_threshold': 0.95  # 95% of optimal performance
}
```

#### Stage 2: Direct Transfer (No Fine-tuning)
```python
direct_transfer = {
    'drone_mass': 5.0,
    'evaluation_episodes': 100,
    'metrics': ['zero-shot performance']
}
```

#### Stage 3: Fine-tuning
```python
finetune_config = {
    'drone_mass': 5.0,
    'training_episodes': 200,  # Reduced from 1000
    'learning_rate_multiplier': 0.1,  # Lower LR for fine-tuning
    'freeze_layers': None  # or ['feature_extractor']
}
```

### 11.2 Domain Randomization

```python
domain_randomization = {
    'mass_range': [0.8, 1.2],  # Â±20% during pre-training
    'inertia_variation': 0.15,  # Â±15%
    'drag_coefficient_range': [0.008, 0.012],
    'motor_time_constant_range': [0.015, 0.025]
}
```

### 11.3 Intermediate Weight Validation

```python
validation_masses = [1.0, 2.0, 3.0, 4.0, 5.0]  # kg
```

---

## 12. Code Structure Requirements

### 12.1 Directory Structure

```
rl_tuned_mpc/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ drone_light.yaml
â”‚   â”œâ”€â”€ drone_heavy.yaml
â”‚   â”œâ”€â”€ mpc_default.yaml
â”‚   â”œâ”€â”€ rl_ppo.yaml
â”‚   â”œâ”€â”€ rl_sac.yaml
â”‚   â”œâ”€â”€ rl_td3.yaml
â”‚   â””â”€â”€ bayesian_opt.yaml
â”‚
â”œâ”€â”€ webots_worlds/
â”‚   â”œâ”€â”€ drone_arena.wbt
â”‚   â”œâ”€â”€ protos/
â”‚   â”‚   â”œâ”€â”€ LightDrone.proto
â”‚   â”‚   â”œâ”€â”€ HeavyDrone.proto
â”‚   â”‚   â””â”€â”€ MediumDrone.proto
â”‚   â””â”€â”€ textures/
â”‚
â”œâ”€â”€ controllers/
â”‚   â”œâ”€â”€ supervisor_controller/
â”‚   â”‚   â”œâ”€â”€ supervisor_controller.py
â”‚   â”‚   â””â”€â”€ episode_manager.py
â”‚   â”œâ”€â”€ mpc_controller/
â”‚   â”‚   â”œâ”€â”€ mpc_controller.py
â”‚   â”‚   â”œâ”€â”€ drone_dynamics.py
â”‚   â”‚   â””â”€â”€ casadi_optimizer.py
â”‚   â””â”€â”€ rl_controller/
â”‚       â”œâ”€â”€ rl_controller.py
â”‚       â””â”€â”€ policy_wrapper.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dynamics/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ quadrotor_model.py
â”‚   â”‚   â””â”€â”€ parameter_loader.py
â”‚   â”œâ”€â”€ mpc/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ mpc_solver.py
â”‚   â”‚   â”œâ”€â”€ cost_function.py
â”‚   â”‚   â””â”€â”€ constraints.py
â”‚   â”œâ”€â”€ environments/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ drone_mpc_env.py
â”‚   â”‚   â”œâ”€â”€ trajectory_generator.py
â”‚   â”‚   â””â”€â”€ reward_functions.py
â”‚   â”œâ”€â”€ rl_agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ppo_agent.py
â”‚   â”‚   â”œâ”€â”€ sac_agent.py
â”‚   â”‚   â”œâ”€â”€ td3_agent.py
â”‚   â”‚   â”œâ”€â”€ a2c_agent.py
â”‚   â”‚   â””â”€â”€ agent_factory.py
â”‚   â”œâ”€â”€ bayesian_opt/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ bo_optimizer.py
â”‚   â”‚   â””â”€â”€ objective_functions.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â”œâ”€â”€ metrics.py
â”‚       â””â”€â”€ visualization.py
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_bo.py
â”‚   â”œâ”€â”€ train_rl_light.py
â”‚   â”œâ”€â”€ train_rl_heavy.py
â”‚   â”œâ”€â”€ transfer_learning.py
â”‚   â””â”€â”€ hyperparameter_tuning.py
â”‚
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_baseline_comparison.py
â”‚   â”œâ”€â”€ run_algorithm_comparison.py
â”‚   â”œâ”€â”€ run_transfer_experiments.py
â”‚   â””â”€â”€ run_ablation_studies.py
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluate_controller.py
â”‚   â”œâ”€â”€ plot_results.py
â”‚   â””â”€â”€ statistical_analysis.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_dynamics.py
â”‚   â”œâ”€â”€ test_mpc.py
â”‚   â”œâ”€â”€ test_environment.py
â”‚   â””â”€â”€ test_agents.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ trajectories/
â”‚   â”œâ”€â”€ trained_models/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ results/
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ API.md
    â”œâ”€â”€ USAGE.md
    â”œâ”€â”€ EXPERIMENTS.md
    â””â”€â”€ RESULTS.md
```

### 12.2 Configuration File Format (YAML)

#### Example: drone_light.yaml
```yaml
drone:
  name: "Lightweight Quadrotor"
  mass: 1.0
  dimensions:
    arm_length: 0.25
    rotor_diameter: 0.15
  inertia:
    Ixx: 0.0123
    Iyy: 0.0123
    Izz: 0.0224
  motors:
    max_thrust: 7.5
    time_constant: 0.02
    max_rpm: 8000
  aerodynamics:
    drag_coefficient: 0.01
```

#### Example: mpc_default.yaml
```yaml
mpc:
  sampling_time: 0.05
  prediction_horizon: 20
  control_horizon: 10
  
  weights:
    Q_position: [100, 100, 100]
    Q_velocity: [10, 10, 10]
    Q_orientation: [50, 50, 20]
    Q_angular_velocity: [1, 1, 1]
    R_control: [1, 1, 1, 1]
    S_rate: [0.1, 0.1, 0.1, 0.1]
  
  constraints:
    max_thrust: 30.0
    max_tilt_angle: 30.0
    max_velocity: 5.0
    min_altitude: 0.5
    max_altitude: 8.0
  
  solver:
    type: "ipopt"
    max_iterations: 100
    tolerance: 1e-6
```

### 12.3 Key Class Definitions

#### MPC Controller
```python
class MPCController:
    def __init__(self, config_path):
        self.config = load_config(config_path)
        self.solver = None
        self.state_trajectory = []
        
    def setup_optimization(self):
        """Setup CasADi optimization problem"""
        pass
    
    def update_parameters(self, Q, R, N_p, N_c):
        """Update MPC tuning parameters"""
        pass
    
    def compute_control(self, current_state, reference):
        """Solve MPC optimization and return control input"""
        pass
```

#### RL Environment
```python
class DroneMPCEnv(gym.Env):
    def __init__(self, config):
        self.config = config
        self.mpc_controller = MPCController(config)
        self.drone_dynamics = QuadrotorModel(config)
        
        self.observation_space = spaces.Box(...)
        self.action_space = spaces.Box(...)
        
    def reset(self):
        """Reset environment and return initial observation"""
        pass
    
    def step(self, action):
        """Execute action and return (obs, reward, done, info)"""
        pass
    
    def render(self, mode='human'):
        """Visualize current state"""
        pass
```

---

## 13. Development Timeline

### Phase 1: Foundation & Baseline (Weeks 1-2)
**Deliverables:**
- [x] Webots environment setup with arena (COMPLETED in published work)
- [x] Four UAV PROTO files created (Crazyflie, Racing, Generic, Heavy-Lift)
- [x] Nonlinear MPC controller with CasADi/IPOPT
- [x] Drone dynamics validated (12-state, 4-input model)
- [ ] Bryson's Rule implementation and validation

**Key Tasks:**
- Install all required software (Webots, CasADi, IPOPT, Stable-Baselines3)
- Create detailed PROTO files matching Table 1 specifications
- Implement and test RK4 integration (Î”t = 0.02s)
- Validate MPC solver performance (~34ms solve time)
- Implement Bryson's Rule initialization for all platforms
- Verify hovering stability for all platforms

**Expected Outcomes:**
- Stable hovering at 2m altitude
- MPC solver converging within tolerance
- Bryson's Rule producing reasonable initial weights

### Phase 2: PPO Baseline Training - Crazyflie (Weeks 3-4)
**Deliverables:**
- [ ] Gymnasium environment with 29D state, 17D action spaces
- [ ] Reward function implementation (Equation 8 from paper)
- [ ] PPO training pipeline with 4 parallel environments
- [ ] Baseline results: 20,000 steps, 1.33m RMSE, 200.3 min training
- [ ] Checkpoint system for resumable training

**Key Tasks:**
- Design state/action space matching published specification
- Implement composite reward function with tuned weights
- Configure PPO: lr=3e-4, n_steps=2048, batch_size=64
- Train Crazyflie baseline until convergence (~18,000 steps)
- Log all metrics to Weights & Biases
- Save best policy checkpoint

**Success Criteria:**
- RMSE â‰¤ 1.35m on circular trajectory
- Training completes within 220 minutes
- No solver failures during training

### Phase 3: Sequential Transfer Learning (Weeks 5-6)
**Deliverables:**
- [ ] Transfer learning module with automatic policy loading
- [ ] Learning rate scheduler (0.1Ã— reduction for fine-tuning)
- [ ] Fine-tuning results for Racing Quadrotor (5,000 steps, 52 min)
- [ ] Fine-tuning results for Generic Quadrotor (5,000 steps, 52 min)
- [ ] Fine-tuning results for Heavy-Lift Hex (5,000 steps, 59 min)
- [ ] Comprehensive performance analysis

**Key Tasks:**
- Implement policy transfer: Ï€_i â† Ï€_{i-1}
- Reduce learning rate: Î·_i = 0.1 Ã— Î·_1
- Fine-tune Racing platform (converge by 5,000 steps)
- Fine-tune Generic platform
- Fine-tune Heavy-Lift platform
- Validate 75% step reduction, 56.2% time reduction
- Verify consistent 1.34Â±0.01m RMSE across all platforms

**Success Criteria:**
- Each platform fine-tunes within 5,000 steps
- RMSE remains within 1.33-1.35m range
- Total training time â‰¤ 400 minutes
- Transfer successful across 200Ã— mass variation

### Phase 4: Alternative RL Algorithms (Weeks 7-9)
**Deliverables:**
- [ ] TRPO implementation and baseline training on Crazyflie
- [ ] SAC implementation and baseline training
- [ ] TD3 implementation and baseline training
- [ ] A2C implementation and baseline training
- [ ] Transfer learning validation for each algorithm
- [ ] Comparative performance analysis

**Key Tasks:**

**Week 7 - TRPO:**
- Implement TRPO with trust region constraint (Î´=0.01)
- Train baseline on Crazyflie (20,000 steps)
- Perform sequential transfer to other platforms
- Compare convergence stability vs PPO
- Measure wall-clock time overhead

**Week 8 - SAC & TD3:**
- Implement SAC with automatic entropy tuning
- Implement TD3 with twin Q-networks
- Train baseline on Crazyflie (40,000 steps expected)
- Fine-tune on remaining platforms (10,000 steps)
- Analyze sample efficiency advantages
- Compare replay buffer memory requirements

**Week 9 - A2C & Analysis:**
- Implement A2C as synchronous baseline
- Train baseline on Crazyflie (30,000 steps expected)
- Complete transfer learning experiments
- Generate comparison tables and plots
- Statistical significance testing (paired t-tests)

**Success Criteria:**
- All algorithms converge to RMSE â‰¤ 1.5m
- Statistical comparison of sample efficiency completed
- Wall-clock time comparison documented
- Transfer learning validated for each algorithm

### Phase 5: Bryson's Rule Ablation Study (Week 10)
**Deliverables:**
- [ ] PPO with Bryson's initialization vs random initialization
- [ ] Convergence speed comparison
- [ ] Analysis of initial weight quality
- [ ] Quantification of sample efficiency improvement

**Key Tasks:**
- Train PPO with random initialization (control)
- Train PPO with Bryson's Rule initialization
- Compare first 1,000-5,000 training steps
- Measure convergence acceleration
- Analyze learned weight distributions
- Compare final performance

**Expected Results:**
- 20-40% faster convergence with Bryson's Rule
- Better initial performance (fewer early failures)
- More interpretable learned weights

### Phase 6: Comprehensive Evaluation & Expert Baseline (Weeks 11-12)

**Deliverables:**
- [ ] Expert manual tuning baseline for all 4 platforms
- [ ] Complete experimental results for all RL algorithms
- [ ] Statistical comparison: RL+Transfer vs. Expert tuning
- [ ] Visualization: training curves, comparison plots
- [ ] Ablation studies: Bryson's vs random init, transfer vs no-transfer
- [ ] Failure case analysis and discussion

**Key Tasks:**

**Week 11 - Expert Tuning Baseline:**
- Recruit control expert or simulate expert tuning process
- Manually tune MPC for each platform using trial-and-error
- Document time spent and intermediate results
- Record RMSE for each platform
- Document expert's thought process and challenges
- Compare expert results with RL+Transfer results

**Week 12 - Comprehensive Analysis:**
- Run 5 independent trials per configuration
- Generate publication-quality comparison figures:
  - Time savings: Expert (16-32h) vs RL+Transfer (6.1h)
  - Performance: Expert RMSE vs RL+Transfer RMSE
  - Consistency: Expert variance vs RL consistency
- Perform statistical tests:
  - Paired t-test: RL vs Expert for each platform
  - Friedman test: All algorithms comparison
  - Effect size calculation (Cohen's d)
- Analyze failure modes and edge cases
- Document computational requirements

**Expert Tuning Protocol:**
```python
expert_tuning_protocol = {
    'participant': 'Control engineer with 3+ years MPC experience',
    'tools_provided': [
        'Webots simulation environment',
        'Test trajectories (hover, circular, waypoint)',
        'Real-time performance feedback',
        'MPC parameter adjustment interface'
    ],
    'constraints': [
        'No access to RL-learned parameters',
        'Must use same test trajectories',
        'Measured by wall-clock time',
        'Can use any tuning strategy (trial-error, Bryson\'s, intuition)'
    ],
    'stopping_criteria': [
        'Expert declares satisfaction with performance',
        'Or maximum 8 hours per platform',
        'Or RMSE converges (< 5% improvement in 30 min)'
    ],
    'metrics_collected': {
        'tuning_time': 'Wall-clock hours',
        'num_iterations': 'How many parameter adjustments',
        'final_rmse': 'Best achieved tracking error',
        'parameter_trajectory': 'History of Q, R, N values',
        'subjective_difficulty': 'Expert rating (1-10 scale)'
    }
}
```

**Success Criteria:**
- Expert tuning data collected for all 4 platforms
- Statistically significant differences identified (p<0.05)
- Publication-ready comparison figures completed
- All claims supported by empirical evidence
- Clear demonstration of RL+Transfer advantages

### Phase 7: Extended Experiments (Week 13)
**Deliverables:**
- [ ] Disturbance rejection tests (wind gusts)
- [ ] Robustness analysis (sensor noise, delays)
- [ ] Different trajectory types validation
- [ ] Performance under payload variations
- [ ] Real-time factor analysis

**Key Tasks:**
- Test with 20-30% wind disturbances
- Evaluate with 2Ã— sensor noise
- Test communication delays (50-100ms)
- Vary payload masses (Â±50%)
- Measure computational overhead

### Phase 8: Documentation & Publication (Weeks 14-16)
**Deliverables:**
- [ ] Complete code documentation
- [ ] User guide with installation instructions
- [ ] Tutorial notebooks (Jupyter)
- [ ] API reference documentation
- [ ] Extended conference/journal paper
- [ ] GitHub repository public release
- [ ] Demo videos for each platform

**Key Tasks:**

**Week 14 - Code Documentation:**
- Document all classes and functions
- Create example scripts for each algorithm
- Write comprehensive README
- Prepare requirements.txt and setup.py
- Add unit tests (>80% coverage)

**Week 15 - User Guide:**
- Installation guide for Windows/Linux/Mac
- Quick start tutorial (10-minute demo)
- Advanced usage examples
- Troubleshooting section
- FAQ based on development experience

**Week 16 - Publication:**
- Write extended paper (8-10 pages)
- Include comprehensive algorithm comparison
- Add Bryson's Rule analysis
- Create supplementary material
- Prepare presentation slides
- Submit to ICRA, IROS, or CDC

### Timeline Summary

| Phase | Duration | Key Milestone | Status |
|-------|----------|---------------|--------|
| 1. Foundation | Weeks 1-2 | MPC + Bryson's Rule | Partially Complete |
| 2. PPO Baseline | Weeks 3-4 | Crazyflie training | Completed |
| 3. Transfer Learning | Weeks 5-6 | 4 platforms validated | Completed |
| 4. Alternative RL | Weeks 7-9 | 5 algorithms compared | To Do |
| 5. Bryson's Ablation | Week 10 | Initialization study | To Do |
| 6. Evaluation | Weeks 11-12 | Statistical analysis | To Do |
| 7. Extended Tests | Week 13 | Robustness validation | To Do |
| 8. Documentation | Weeks 14-16 | Publication ready | To Do |

**Total Duration:** 16 weeks (~4 months)

**Critical Path:**
1. Foundation (2 weeks) â†’ PPO Baseline (2 weeks) â†’ Transfer Learning (2 weeks)
2. Alternative RL (3 weeks) â†’ Evaluation (2 weeks) â†’ Publication (3 weeks)

**Parallel Work Opportunities:**
- Documentation can begin during evaluation phase
- Bryson's ablation can run parallel with alternative RL algorithms
- Video recording throughout all phases

---

## 14. Success Criteria

### 14.1 Technical Criteria (Based on Published Results)

| Criterion | Target | Achieved (Published) | Measurement |
|-----------|--------|---------------------|-------------|
| PPO baseline RMSE | < 1.35m | **1.33m** âœ“ | Crazyflie circular trajectory |
| Cross-platform RMSE consistency | Â± 0.02m | **1.34Â±0.01m** âœ“ | All 4 platforms |
| Training step reduction | > 70% | **75%** (20kâ†’5k) âœ“ | Fine-tuning vs baseline |
| Training time reduction | > 50% | **56.2%** (801â†’363 min) âœ“ | Total time across 4 platforms |
| Transfer learning success | All platforms | **4/4 platforms** âœ“ | Successful convergence |
| MPC solver real-time | < 50ms | **~34ms** âœ“ | Average solve time |
| Control frequency | 50 Hz | **50 Hz** âœ“ | Î”t = 0.02s |
| Mass range generalization | 100Ã— minimum | **200Ã—** (0.027-5.5kg) âœ“ | Platform range |
| Parallel speedup | > 3Ã— | **~3.7Ã—** âœ“ | 4 parallel envs |
| Total training time | < 8 hours | **6.1 hours** âœ“ | Wall-clock time |

### 14.2 Algorithm Comparison Criteria (To Be Validated)

#### Sample Efficiency Targets
| Algorithm | Expected Base Steps | Expected Fine-Tune Steps | Expected RMSE |
|-----------|--------------------|-----------------------|---------------|
| **PPO** (proven) | 20,000 | 5,000 | 1.33-1.34m |
| TRPO | 20,000-25,000 | 5,000-6,000 | 1.33-1.35m |
| SAC | 30,000-40,000 | 8,000-10,000 | 1.30-1.35m |
| TD3 | 30,000-40,000 | 8,000-10,000 | 1.32-1.36m |
| A2C | 25,000-35,000 | 6,000-8,000 | 1.35-1.40m |

#### Wall-Clock Time Targets
| Algorithm | Base Training (min) | Per-Platform Fine-Tune (min) | Total Time (4 platforms) |
|-----------|---------------------|------------------------------|--------------------------|
| **PPO** (proven) | 200 | 52-59 | 363 min (6.1 hrs) |
| TRPO | 300-400 | 70-90 | 500-700 min |
| SAC | 350-450 | 90-120 | 600-800 min |
| TD3 | 350-450 | 90-120 | 600-800 min |
| A2C | 250-350 | 60-80 | 450-600 min |

### 14.3 Bryson's Rule Validation Criteria

| Metric | Random Init | Bryson's Init | Target Improvement |
|--------|-------------|---------------|-------------------|
| Steps to convergence | 20,000-25,000 | 15,000-20,000 | 20-30% reduction |
| Initial RMSE (step 0) | 5-10m | 2-4m | 50% better |
| Training stability (std) | 3-5 | 2-3 | Lower variance |
| Final RMSE | 1.33-1.35m | 1.32-1.34m | Comparable |
| Convergence speed | Baseline | 20-40% faster | Significant (p<0.05) |

### 14.4 Research Contribution Criteria

**Primary Contributions (Must Achieve):**
- [x] Demonstrate 75% training step reduction via sequential transfer learning âœ“
- [x] Validate consistent performance (1.34Â±0.01m) across 200Ã— mass variation âœ“
- [x] Achieve sub-400 minute total training time for 4 platforms âœ“
- [ ] Compare 5 RL algorithms for MPC hyperparameter tuning
- [ ] Validate Bryson's Rule initialization benefits
- [ ] Provide open-source, reproducible framework

**Secondary Contributions (Should Achieve):**
- [ ] Identify best RL algorithm for MPC tuning tasks
- [ ] Quantify sample efficiency vs wall-clock time tradeoffs
- [ ] Demonstrate robustness to disturbances and uncertainties
- [ ] Show generalization to unseen trajectory types
- [ ] Establish benchmark dataset for MPC-RL research

**Publication Criteria:**
- Conference paper accepted at ICRA/IROS/CDC (Tier 1 robotics)
- Code released on GitHub with >50 stars within 6 months
- Benchmark dataset used by â‰¥3 other research groups
- Extensions to journal paper (e.g., IEEE T-RO, Automatica)

### 14.5 Practical Deployment Criteria

**Reliability:**
- [ ] Training completes without manual intervention (checkpoint-based)
- [ ] Success rate >95% across 10 independent training runs
- [ ] Robust to common failures (solver timeouts, simulation crashes)

**Reproducibility:**
- [ ] Same random seeds produce identical results (Â±1% variance)
- [ ] Installation works on Windows, Linux, macOS
- [ ] Dependencies version-pinned and conflict-free
- [ ] Documentation enables replication within 1 day

**Scalability:**
- [ ] Framework extends to 6+ platforms without modification
- [ ] Supports custom drone configurations via YAML
- [ ] Parallel training scales linearly up to 8 environments
- [ ] Memory footprint <16GB RAM

**Usability:**
- [ ] Quick-start example runs in <10 minutes
- [ ] Training visualizations auto-generated (W&B/TensorBoard)
- [ ] Error messages clear and actionable
- [ ] API documentation >90% coverage

### 14.6 Performance Benchmarks (Quantitative Targets)

#### Tracking Performance
```python
tracking_benchmarks = {
    'hover': {
        'rmse_target': 0.05,      # m (should be very accurate)
        'settling_time': 3.0,     # seconds
        'overshoot': 0.10         # m
    },
    'circular': {
        'rmse_target': 1.34,      # m (primary metric)
        'max_error': 2.5,         # m
        'completion': True        # Must complete full circle
    },
    'waypoint': {
        'rmse_target': 1.50,      # m (slightly higher tolerance)
        'arrival_threshold': 0.3, # m (waypoint proximity)
        'completion_time': 20     # seconds
    },
    'lemniscate': {
        'rmse_target': 1.80,      # m (aggressive maneuver)
        'max_tilt': 30,           # degrees (safety)
        'completion': True        # Must complete figure-8
    }
}
```

#### Training Performance
```python
training_benchmarks = {
    'convergence': {
        'ppo_steps': 20000,
        'max_steps': 50000,           # Failure if not converged
        'reward_threshold': -5.0       # Episode reward target
    },
    'stability': {
        'reward_std': 2.0,            # Stable training
        'success_rate': 0.90,         # 90% episodes successful
        'solver_failures': 0.02       # <2% solver timeouts
    },
    'efficiency': {
        'samples_per_second': 500,    # With 4 parallel envs
        'training_hours_per_platform': 3.5,  # Wall-clock
        'gpu_optional': True          # Should work without GPU
    }
}
```

### 14.7 Failure Criteria (Project Risks)

**Critical Failures (Project Termination):**
- Training time >15 hours for 4 platforms
- RMSE >2.0m on any platform
- Transfer learning completely fails (no improvement)
- Simulation instabilities prevent training

**Major Issues (Require Mitigation):**
- Algorithm comparison shows <10% performance difference
- Bryson's Rule provides <10% improvement
- Training requires manual intervention >5 times
- Code cannot run on consumer hardware

**Minor Issues (Document and Report):**
- Slightly longer training times than published
- Small RMSE variations (1.30-1.38m range)
- Some algorithms don't transfer as well
- Documentation gaps or unclear instructions

---

## 15. Risk Mitigation

### 15.1 Technical Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| RL training instability | High | High | Extensive hyperparameter tuning, use proven algorithms |
| Transfer learning failure | Medium | High | Domain randomization during pre-training |
| Webots-Python interface issues | Medium | Medium | Fallback to PyBullet if needed |
| Computational resource limits | Medium | Medium | Cloud computing (AWS, Google Cloud) |
| MPC solver convergence issues | Low | High | Multiple solver options (IPOPT, qpOASES) |

### 15.2 Timeline Risks

| Risk | Mitigation |
|------|------------|
| Scope creep | Stick to core objectives, defer advanced features |
| Debugging delays | Implement comprehensive unit tests early |
| Experiment runtime | Use parallel simulations, start experiments early |

---

## 16. Deliverables Checklist

### Code Deliverables
- [ ] Complete source code with documentation
- [ ] Unit tests (>80% coverage)
- [ ] Example scripts and tutorials
- [ ] Pre-trained model weights
- [ ] Configuration files for all experiments

### Data Deliverables
- [ ] Training logs and metrics
- [ ] Evaluation results (CSV/JSON)
- [ ] Trained model checkpoints
- [ ] Video recordings of successful flights

### Documentation Deliverables
- [ ] Technical documentation (API reference)
- [ ] User guide with examples
- [ ] Experiment reproduction guide
- [ ] Troubleshooting guide

### Academic Deliverables
- [ ] Conference paper (6-8 pages)
- [ ] Supplementary material
- [ ] Poster/presentation slides
- [ ] Demo video for submission

---

## 17. Installation Instructions

### 17.1 System Setup

```bash
# 1. Clone repository
git clone https://github.com/yourusername/rl_tuned_mpc.git
cd rl_tuned_mpc

# 2. Create virtual environment
python -m venv venv_rl_mpc
source venv_rl_mpc/bin/activate  # Linux/Mac
# or
venv_rl_mpc\Scripts\activate  # Windows

# 3. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# 4. Install Webots (download from official website)
# https://cyberbotics.com/

# 5. Verify installation
python -c "import casadi; import torch; import gymnasium; print('All imports successful')"
```

### 17.2 Requirements.txt

```
# Core dependencies
numpy>=1.23.0
scipy>=1.10.0
matplotlib>=3.7.0
pandas>=2.0.0
seaborn>=0.12.0

# Simulation & Control
casadi>=3.6.0
pybullet>=3.2.0

# Reinforcement Learning
gymnasium>=0.29.0
stable-baselines3[extra]>=2.0.0
torch>=2.0.0
torchvision>=0.15.0

# Bayesian Optimization
botorch>=0.9.0
gpyopt>=1.2.6
optuna>=3.3.0

# Experiment tracking
wandb>=0.15.0
tensorboard>=2.13.0

# Utilities
pyyaml>=6.0
tqdm>=4.65.0
plotly>=5.16.0

# Development
pytest>=7.4.0
black>=23.0.0
pylint>=2.17.0
jupyter>=1.0.0
```

---

## 18. Frequently Asked Questions (FAQ)

### Q1: Why use MPC + RL instead of pure RL?
**A:** MPC provides safety guarantees, interpretability, and incorporates domain knowledge (physics, constraints). RL enhances MPC by optimizing parameters that are traditionally hand-tuned.

### Q2: Why Bayesian Optimization as a baseline?
**A:** BO is a state-of-the-art black-box optimization method for expensive objectives (like simulation runs). It provides a strong baseline for comparison.

### Q3: Can this approach work on real drones?
**A:** Yes, but sim-to-real transfer requires additional steps: system identification, domain randomization, and possibly reality gap modeling.

### Q4: How long does training take?
**A:** On a modern GPU (RTX 3070), expect:
- BO tuning: 4-8 hours (50 iterations)
- RL training (light): 12-24 hours (1M steps)
- RL fine-tuning (heavy): 4-8 hours (200k steps)

### Q5: What if my drone has different dynamics?
**A:** The framework is modular - simply update the drone configuration YAML files and dynamics model in `src/dynamics/quadrotor_model.py`.

---

## 19. References & Resources

### Key Papers (Project Foundation)

**Published Foundation:**
1. **Your Published Work (AAAI 2025):**
   - Khan, A. M., "Reinforcement Learning-Enhanced Model Predictive Control with Sequential Transfer Learning for Multi-UAV Systems", AAAI Conference, 2025
   - **Key Results:** 1.34Â±0.01m RMSE, 75% step reduction, 56.2% time reduction
   - **Validated Across:** 200Ã— mass variation (0.027kg - 5.5kg)

**Bryson's Rule Foundation:**
2. **Bryson, A. E., & Ho, Y. C.** (1975). *Applied Optimal Control: Optimization, Estimation, and Control*. Hemisphere Publishing Corporation.
   - Original formulation of Bryson's Rule for weight matrix selection
   - Chapter 5: "Optimal Control of Linear Systems"

3. **Anderson, B. D., & Moore, J. B.** (2007). *Optimal Control: Linear Quadratic Methods*. Dover Publications.
   - Section 2.4: "Choosing Weighting Matrices" (practical application)

**MPC for Quadrotors:**
4. **Alexis, K., Nikolakopoulos, G., & Tzes, A.** (2016). "Model predictive quadrotor control: attitude, altitude and position experimental studies." *IET Control Theory & Applications*, 6(12), 1812-1827.
   - Linear MPC formulation for attitude and position control

5. **Bangura, M., & Mahony, R.** (2014). "Nonlinear dynamic modeling for high performance control of a quadrotor." *Australasian Conference on Robotics and Automation (ACRA)*.
   - Nonlinear dynamics modeling and validation

6. **Kamel, M., Burri, M., & Siegwart, R.** (2017). "Linear vs nonlinear MPC for trajectory tracking applied to rotary wing micro aerial vehicles." *IFAC-PapersOnLine*, 50(1), 3463-3469.
   - Comparative study of linear and nonlinear MPC

**Reinforcement Learning for Control:**
7. **Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O.** (2017). "Proximal policy optimization algorithms." *arXiv preprint arXiv:1707.06347*.
   - PPO algorithm (primary method used)

8. **Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S.** (2018). "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor." *ICML*.
   - SAC algorithm for continuous control

9. **Fujimoto, S., Hoof, H., & Meger, D.** (2018). "Addressing function approximation error in actor-critic methods." *ICML*.
   - TD3 algorithm addressing overestimation

10. **Schulman, J., Levine, S., Abbeel, P., Jordan, M., & Moritz, P.** (2015). "Trust region policy optimization." *ICML*.
    - TRPO with monotonic improvement guarantee

**RL-Enhanced MPC:**
11. **Mehndiratta, M., Kayacan, E., Reyhanoglu, M., Kayacan, E., & Chowdhary, G.** (2020). "Automated tuning of nonlinear MPC for quadrotors using reinforcement learning." *IEEE International Conference on Robotics and Automation (ICRA)*.
    - Prior work on RL-based MPC tuning (single platform)

12. **Hewing, L., Wabersich, K. P., Menner, M., & Zeilinger, M. N.** (2020). "Learning-based model predictive control: Toward safe learning in control." *Annual Review of Control, Robotics, and Autonomous Systems*, 3, 269-296.
    - Comprehensive survey of learning-enhanced MPC

**Transfer Learning in Robotics:**
13. **Taylor, M. E., & Stone, P.** (2009). "Transfer learning for reinforcement learning domains: A survey." *Journal of Machine Learning Research*, 10(7), 1633-1685.
    - Foundational survey on transfer learning

14. **Liu, H., Socher, R., & Li, F. F.** (2019). "Multi-task deep reinforcement learning with PopArt normalization for robotic manipulation." *arXiv preprint arXiv:1809.02591*.
    - Multi-task RL with population-based training

15. **Yu, T., et al.** (2020). "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning." *Conference on Robot Learning (CoRL)*.
    - Benchmark for multi-task RL

16. **Berkenkamp, F., Krause, A., & Schoellig, A. P.** (2017). "Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics." *Machine Learning*, 112, 1-35.
    - Safe Bayesian optimization for robot tuning

**Optimization and Solvers:**
17. **Andersson, J. A., Gillis, J., Horn, G., Rawlings, J. B., & Diehl, M.** (2019). "CasADi: a software framework for nonlinear optimization and optimal control." *Mathematical Programming Computation*, 11(1), 1-36.
    - CasADi framework for symbolic computation

18. **WÃ¤chter, A., & Biegler, L. T.** (2006). "On the implementation of an interior-point filter line-search algorithm for large-scale nonlinear programming." *Mathematical Programming*, 106(1), 25-57.
    - IPOPT solver used in this work

**Multi-UAV Systems:**
19. **Mohsan, S. A. H., et al.** (2023). "Unmanned aerial vehicles (UAVs): Practical aspects, applications, open challenges, security issues, and future trends." *Intelligent Service Robotics*, 16(1), 109-137.
    - Recent survey on UAV applications

20. **Zhou, X., et al.** (2020). "Swarm of micro flying robots in the wild." *Science Robotics*, 5(42).
    - Heterogeneous swarm control challenges

### Useful Resources

**Software Documentation:**
- **CasADi:** https://web.casadi.org/
  - Tutorials on optimal control and MPC
  - Python API reference
  
- **Stable-Baselines3:** https://stable-baselines3.readthedocs.io/
  - RL algorithm implementations
  - Training best practices
  
- **Webots:** https://cyberbotics.com/doc/guide/tutorials
  - Robot simulation tutorials
  - PROTO file creation guide
  
- **IPOPT:** https://coin-or.github.io/Ipopt/
  - Solver options and tuning
  - Performance optimization

**Online Courses:**
- **Underactuated Robotics** (MIT 6.832): http://underactuated.mit.edu/
  - Excellent coverage of optimal control and MPC
  
- **Deep RL Course** (UC Berkeley CS285): http://rail.eecs.berkeley.edu/deeprlcourse/
  - Comprehensive RL algorithms coverage

**Datasets and Benchmarks:**
- **AirSim:** https://github.com/microsoft/AirSim
  - UAV simulation environment (alternative)
  
- **RotorS:** https://github.com/ethz-asl/rotors_simulator
  - Gazebo-based MAV simulator

**Community Resources:**
- **Papers with Code:** https://paperswithcode.com/task/model-predictive-control
  - Latest MPC research and implementations
  
- **Reddit r/reinforcementlearning:** Active community for RL discussions

### Implementation Examples

**GitHub Repositories (Similar Work):**
1. **gym-pybullet-drones:** https://github.com/utiasDSL/gym-pybullet-drones
   - PyBullet drone environments for RL
   
2. **mpc-reinforcement-learning:** https://github.com/aravindr93/mpc-rl
   - MPC-guided RL implementations

3. **safe-control-gym:** https://github.com/utiasDSL/safe-control-gym
   - Safety-critical RL for control systems

**Your Project Repository (To Be Released):**
- **URL:** [To be published]
- **Contents:**
  - Complete implementation of 5 RL algorithms
  - Webots PROTO files for all 4 platforms
  - Pre-trained model checkpoints
  - Reproduction scripts
  - Benchmark dataset

---

## 20. Contact & Support

**Project Lead:** Dr. Abdul Manan Khan  
**Email:** [your.email@uwl.ac.uk]  
**GitHub:** [repository URL]

**For Issues:**
- Technical problems: Open GitHub issue
- Research questions: Email project lead
- Collaboration inquiries: Email project lead

---

## Document Revision History

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | Nov 2025 | Dr. Abdul Manan Khan | Initial requirements document |
| 2.0 | Nov 2025 | Dr. Abdul Manan Khan | **Major Update:** Incorporated published AAAI paper results, added Bryson's Rule initialization, expanded to 5 RL algorithms (PPO, TRPO, SAC, TD3, A2C), updated all platforms to match published specifications (Crazyflie 2.X, Racing, Generic, Heavy-Lift), validated performance targets (1.34Â±0.01m RMSE, 75% step reduction, 56.2% time reduction), comprehensive algorithm comparison framework |

**Key Changes in Version 2.0:**
- âœ“ Replaced generic drone platforms with validated 4-platform configuration
- âœ“ Added Bryson's Rule theoretical foundation and implementation guidelines
- âœ“ Expanded from 4 to 5 RL algorithms with detailed specifications
- âœ“ Incorporated actual experimental results from published research
- âœ“ Updated success criteria to match demonstrated performance
- âœ“ Added comprehensive algorithm comparison framework
- âœ“ Included detailed Bryson's Rule ablation study requirements
- âœ“ Updated timeline to reflect partially completed work
- âœ“ Added 20+ citations including original Bryson & Ho reference

---

**End of Requirements Document**
